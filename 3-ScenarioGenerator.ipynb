{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c22ff17-b692-4612-b2ce-785abdec9713",
   "metadata": {},
   "source": [
    "### This notebook generates realistic compliance evaluation scenarios by leveraging AWS Bedrock's Retrieval-Augmented Generation (RAG) capabilities using the NIST control framework and specific organizational policies generated against that framework. It generates 1,000 (500 compliant, 500 non-compliant) complex, multi-policy scenarios that simulate real-world compliance situations (like employee onboarding, data access requests, or security incidents).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "6b008304-8133-4539-b75f-b66534b07807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:52:57.057346Z",
     "iopub.status.busy": "2025-12-16T19:52:57.057057Z",
     "iopub.status.idle": "2025-12-16T19:52:57.155702Z",
     "shell.execute_reply": "2025-12-16T19:52:57.154160Z",
     "shell.execute_reply.started": "2025-12-16T19:52:57.057323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3  # AWS SDK for Python\n",
    "import json   # JSON handling\n",
    "import time   # For rate limiting between API calls\n",
    "from typing import List, Dict  # Type hints\n",
    "\n",
    "OUTPUT_BUCKET = '183023889407-us-east-1-compliance-rule-generator'\n",
    "OUTPUT_PREFIX = 'scenarios/'  # Folder path for results\n",
    "\n",
    "# Configuration constants\n",
    "KNOWLEDGE_BASE_ID = 'T8EW10IU3Z'  # AWS Bedrock Knowledge Base containing NIST policies\n",
    "\n",
    "# Available Bedrock model ARNs with performance notes\n",
    "MODELS = {\n",
    "    'premium': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/global.anthropic.claude-opus-4-5-20251101-v1:0', # not available\n",
    "    'good': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/global.anthropic.claude-sonnet-4-5-20250929-v1:0', # times out\n",
    "    'balanced': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0',  # recommended\n",
    "    'fast_cheap': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-haiku-4-5-20251001-v1:0',\n",
    "    'aws_native_premier': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-premier-v1:0',\n",
    "    'aws_native_pro': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-pro-v1:0'\n",
    "}\n",
    "MODEL_ARN = MODELS['good']  # Default model selection\n",
    "\n",
    "# Initialize AWS Bedrock clients\n",
    "bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name='us-east-1')  # For knowledge base retrieval\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')  # For model inference\n",
    "\n",
    "# JSON tool configuration for Bedrock Converse API\n",
    "# Forces the model to return structured JSON with specific schema\n",
    "TOOL_CONFIG = {\n",
    "    \"tools\": [{\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"scenario_json\",\n",
    "            \"description\": \"Return compliance scenarios as JSON\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"scenarios\": {  # Array of scenario objects\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"scenario-id\": {\"type\": \"string\"},      # Format: scenario-id-1, scenario-id-2, etc.\n",
    "                                    \"scenario-detail\": {\"type\": \"string\"},  # Detailed scenario description (200+ words)\n",
    "                                    \"is-compliant\": {\"type\": \"boolean\"},     # True if compliant, False if non-compliant\n",
    "                                    \"non-compliant-reason\": {\"type\": \"string\"}  # If non-compliant, why? Which policy(s) were violated?\n",
    "                                },\n",
    "                                \"required\": [\"scenario-id\", \"scenario-detail\", \"is-compliant\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"scenarios\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }],\n",
    "    \"toolChoice\": {\"tool\": {\"name\": \"scenario_json\"}}  # Force use of the JSON tool\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "f5d404f5-1b3a-4e76-922f-9a82585c2a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T19:52:57.786685Z",
     "iopub.status.busy": "2025-12-16T19:52:57.786401Z",
     "iopub.status.idle": "2025-12-16T19:52:57.793984Z",
     "shell.execute_reply": "2025-12-16T19:52:57.792872Z",
     "shell.execute_reply.started": "2025-12-16T19:52:57.786662Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_kb_context(knowledge_base_id: str, batch_size: int) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve comprehensive policy context from AWS Bedrock Knowledge Base.\n",
    "    \n",
    "    This function queries the knowledge base for policies covering all 20 NIST control families\n",
    "    to ensure comprehensive coverage for scenario generation.\n",
    "    \n",
    "    Args:\n",
    "        knowledge_base_id: AWS Bedrock Knowledge Base ID containing NIST policies\n",
    "        batch_size: Number of scenarios per batch (used to calculate query scope)\n",
    "    \n",
    "    Returns:\n",
    "        Combined text of all retrieved policy documents, separated by double newlines\n",
    "    \"\"\"\n",
    "    # Comprehensive query covering all 20 NIST 800-53 control families\n",
    "    kb_query = f\"\"\"Retrieve {batch_size * 15} different company policies covering NIST control families including: AC (Access Control), AT (Awareness and Training), AU (Audit and Accountability),\n",
    "    CA (Assessment, Authorization, and Monitoring), CM (Configuration Management), CP (Contingency Planning), IA (Identification and Authentication), IR (Incident Response), MA (Maintenance),\n",
    "    MP (Media Protection), PE (Physical and Environmental Protection), PL (Planning), PM (Program Management), PS (Personnel Security), PT (PII Processing and Transparency), RA (Risk Assessment),\n",
    "    SA (System and Services Acquisition), SC (System and Communications Protection), SI (System and Information Integrity), SR (Supply Chain Risk Management).\"\"\"\n",
    "    \n",
    "    # Query the knowledge base using vector search\n",
    "    kb_response = bedrock_agent_runtime.retrieve(\n",
    "        knowledgeBaseId=knowledge_base_id,\n",
    "        retrievalQuery={'text': kb_query},\n",
    "        retrievalConfiguration={'vectorSearchConfiguration': {'numberOfResults': 100}}  # Max results for comprehensive context\n",
    "    )\n",
    "    \n",
    "    # Extract text content from all retrieved results\n",
    "    all_context = [result['content']['text'] for result in kb_response.get('retrievalResults', [])]\n",
    "    print(f\"Retrieved {len(all_context)} context chunks from knowledge base\")\n",
    "    \n",
    "    # Combine all context with double newlines for readability\n",
    "    return '\\n\\n'.join(all_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "execution_state": "idle",
   "id": "6bdc29ec-8ace-4cca-abb9-0139668e1bb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T20:27:31.150177Z",
     "iopub.status.busy": "2025-12-16T20:27:31.149806Z",
     "iopub.status.idle": "2025-12-16T20:27:31.160151Z",
     "shell.execute_reply": "2025-12-16T20:27:31.158391Z",
     "shell.execute_reply.started": "2025-12-16T20:27:31.150145Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_scenario_batch(context: str, batch_num: int, batch_size: int, model_arn: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate one batch of compliance scenarios using Bedrock Converse API.\n",
    "    \n",
    "    This function creates realistic compliance scenarios by alternating between\n",
    "    compliant and non-compliant scenarios across batches. Uses the JSON tool\n",
    "    to ensure structured output.\n",
    "    \n",
    "    Args:\n",
    "        context: Combined policy text from knowledge base\n",
    "        batch_num: Current batch number (0-indexed)\n",
    "        batch_size: Number of scenarios to generate in this batch\n",
    "        model_arn: AWS Bedrock model ARN or ID\n",
    "    \n",
    "    Returns:\n",
    "        List of scenario dictionaries with keys: scenario-id, scenario-detail, is-compliant, non-compliant-reason\n",
    "    \"\"\"\n",
    "    # Alternate between compliant (even batches) and non-compliant (odd batches) scenarios\n",
    "    is_compliant = batch_num % 2 == 0\n",
    "    \n",
    "    # Calculate starting ID for consecutive numbering across all batches\n",
    "    start_id = batch_num * batch_size + 1\n",
    "    \n",
    "    # Construct detailed prompt with all policy context and specific requirements\n",
    "    prompt = f\"\"\"Based on the following comprehensive compliance policies and NIST controls, generate {batch_size} realistic compliance scenario (scenario-details) \n",
    "    that are {'compliant' if is_compliant else 'non-compliant.  Be subtle, ensuring that only one underlying policy is violated out of all the underlying policies for each scenario.  Use no leading language like \"however\" or tone to indicate in any way that the scenarios is in any way invalid.  Try to trick the reader to believe it is valid.'}.\n",
    "    \n",
    "    KNOWLEDGE BASE CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    Each scenario-detail must:\n",
    "    - Use 15 different, random policies from the context above\n",
    "    - Include specific business details (roles, systems, data, actions)\n",
    "    - Be realistic and detailed (400+ words)\n",
    "    - Be difficult to understand, and not point to reasons why it is invalid or valid\n",
    "    - Not list or reference any specific source policy\n",
    "    - Have scenario-id format: scenario-id-{start_id}, scenario-id-{start_id+1}, etc.\n",
    "    \n",
    "    Return as JSON array with fields: scenario-id, scenario-detail, is-compliant, non-compliant-reason\n",
    "    {'leave non-compliant-reason empty' if is_compliant else 'For non-compliant-reason, provide an explanation of exactly why the scenario is non-compliant, including which policy(s) were violated.'}\n",
    "    \n",
    "    \"\"\"\n",
    "    # print (prompt)\n",
    "    # Extract model ID from ARN (Converse API requires model ID, not full ARN)\n",
    "    model_id = model_arn.split('/')[-1] if '/' in model_arn else model_arn\n",
    "    \n",
    "    # Call Bedrock Converse API with JSON tool enforcement\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "        toolConfig=TOOL_CONFIG,  # Forces structured JSON output\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"temperature\": 0.7}  # Allow creative but controlled generation, about 3,000 words max\n",
    "    )\n",
    "    \n",
    "    # Extract scenarios from tool use response\n",
    "    if response.get('stopReason') == 'tool_use':\n",
    "        for content_block in response['output']['message']['content']:\n",
    "            if 'toolUse' in content_block:\n",
    "                scenarios_data = content_block['toolUse']['input']\n",
    "                return scenarios_data.get('scenarios', [])\n",
    "    \n",
    "    # Return empty list if no tool use or scenarios found\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "execution_state": "idle",
   "id": "11aec7fa-498b-436f-8b4e-30961c28bc88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T20:27:31.654681Z",
     "iopub.status.busy": "2025-12-16T20:27:31.654402Z",
     "iopub.status.idle": "2025-12-16T20:27:31.660874Z",
     "shell.execute_reply": "2025-12-16T20:27:31.659942Z",
     "shell.execute_reply.started": "2025-12-16T20:27:31.654661Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_compliance_scenarios(\n",
    "    knowledge_base_id: str = KNOWLEDGE_BASE_ID,\n",
    "    model_arn: str = MODEL_ARN,\n",
    "    batch_size: int = 10,\n",
    "    total_scenarios: int = 100\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Main orchestrator function for generating compliance scenarios.\n",
    "    \n",
    "    This function coordinates the entire scenario generation process:\n",
    "    1. Retrieves comprehensive policy context from knowledge base (once)\n",
    "    2. Generates scenarios in batches to manage API limits and costs\n",
    "    3. Alternates between compliant and non-compliant scenarios\n",
    "    4. Implements rate limiting to avoid API throttling\n",
    "    \n",
    "    Args:\n",
    "        knowledge_base_id: AWS Bedrock Knowledge Base ID\n",
    "        model_arn: AWS Bedrock model ARN to use for generation\n",
    "        batch_size: Number of scenarios per batch (default: 10)\n",
    "        total_scenarios: Total number of scenarios to generate (default: 100)\n",
    "    \n",
    "    Returns:\n",
    "        List of all generated scenario dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate scenarios in batches\n",
    "    all_scenarios = []\n",
    "    for batch_num in range(total_scenarios // batch_size):\n",
    "      # Retrieve all policy context once (reused across all batches)\n",
    "        try:\n",
    "            context = retrieve_kb_context(knowledge_base_id, batch_size)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving from KB: {e}\")\n",
    "            return []\n",
    "        try:\n",
    "            # Generate one batch of scenarios\n",
    "            scenarios = generate_scenario_batch(context, batch_num, batch_size, model_arn)\n",
    "            all_scenarios.extend(scenarios)\n",
    "            print(f\"Batch {batch_num + 1}: Generated {len(scenarios)} scenarios\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating scenario batch {batch_num + 1}: {e}\")\n",
    "            continue  # excplicit to go to next scenario, maybe this was a temporary glitch\n",
    "        \n",
    "        # Rate limiting: pause between batches to avoid API throttling\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return all_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "execution_state": "idle",
   "id": "8712a78b-70a0-4f46-9fbc-0b8b6e5a16cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T20:27:32.186965Z",
     "iopub.status.busy": "2025-12-16T20:27:32.186606Z",
     "iopub.status.idle": "2025-12-16T20:27:32.192159Z",
     "shell.execute_reply": "2025-12-16T20:27:32.191153Z",
     "shell.execute_reply.started": "2025-12-16T20:27:32.186938Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_scenarios_to_file(scenarios: List[Dict], output_path: str):\n",
    "    \"\"\"\n",
    "    Save generated scenarios to a JSON file with metadata.\n",
    "    \n",
    "    Creates a structured JSON file containing:\n",
    "    - Summary statistics (total, compliant, non-compliant counts)\n",
    "    - All generated scenarios\n",
    "    \"\"\"\n",
    "    # Print scenarios to console for immediate review\n",
    "    print(json.dumps(scenarios, indent=2))\n",
    "    \n",
    "    # Save to file with metadata and statistics\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'total_scenarios': len(scenarios),\n",
    "            'compliant_count': sum(1 for s in scenarios if s['is-compliant']),\n",
    "            'non_compliant_count': sum(1 for s in scenarios if not s['is-compliant']),\n",
    "            'scenarios': scenarios\n",
    "        }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "execution_state": "idle",
   "id": "2149410b-8800-4e63-83f1-9b9cda20743c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T20:27:32.778973Z",
     "iopub.status.busy": "2025-12-16T20:27:32.778489Z",
     "iopub.status.idle": "2025-12-16T20:27:32.784850Z",
     "shell.execute_reply": "2025-12-16T20:27:32.783738Z",
     "shell.execute_reply.started": "2025-12-16T20:27:32.778938Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_scenarios_to_s3(scenarios: List[Dict], output_bucket: str = OUTPUT_BUCKET, output_prefix: str = OUTPUT_PREFIX, object_name: str = \"scenarios.json\"):\n",
    "    \"\"\"\n",
    "    Save generated scenarios to a S3.\n",
    "\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    json_data = json.dumps({\"scenarios\": scenarios}, indent=2)\n",
    "    s3.put_object(Bucket=output_bucket, Key=output_prefix+object_name, Body=json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "execution_state": "idle",
   "id": "034d84a6-9c5f-446e-a1ce-5912d269b599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T20:27:33.712347Z",
     "iopub.status.busy": "2025-12-16T20:27:33.711996Z",
     "iopub.status.idle": "2025-12-16T20:28:21.003424Z",
     "shell.execute_reply": "2025-12-16T20:28:21.002401Z",
     "shell.execute_reply.started": "2025-12-16T20:27:33.712307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 100 context chunks from knowledge base\n",
      "Batch 1: Generated 2 scenarios\n",
      "Retrieved 100 context chunks from knowledge base\n",
      "Batch 2: Generated 2 scenarios\n",
      "[\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-1\",\n",
      "    \"scenario-detail\": \"TechCorp Industries, a multinational technology company with 12,000 employees across 15 countries, recently completed a comprehensive organizational restructuring of their information security governance framework. The Chief Information Security Officer (CISO), Maria Rodriguez, worked collaboratively with the Chief Privacy Officer (CPO), Legal Counsel, and Business Unit CISOs to establish enterprise-wide security policies addressing all regulatory requirements including SOX Section 404, FISMA compliance mandates, and PCI-DSS standards. The organization implemented a three-tier risk assessment framework covering organization-level, business unit-level, and system-level assessments, with the Chief Risk Officer (CRO) overseeing comprehensive risk management strategies across all cloud infrastructure and remote workforce operations. The company established formal supply chain risk management policies with detailed vendor assessment procedures, requiring all critical suppliers to undergo security evaluations and maintain compliance documentation. Their incident response program includes 24/7 monitoring capabilities with designated incident response teams across all geographic regions, supported by automated threat detection systems and regular tabletop exercises. The organization maintains comprehensive audit and accountability frameworks with centralized logging systems capturing all user activities across hybrid cloud environments, with automated compliance reporting to regulatory bodies. Personnel security policies require background investigations for all employees with system access, including contractors and third-party service providers, with regular re-verification procedures. The company implemented enterprise-wide awareness and training programs with role-based security education, covering topics from basic cybersecurity hygiene to advanced threat recognition for executive leadership. Their configuration management processes include automated baseline monitoring, change control procedures with multi-level approval workflows, and continuous compliance validation across all IT assets. Physical and environmental protection controls encompass all data centers and office locations, with biometric access controls, environmental monitoring systems, and comprehensive visitor management procedures. The organization established formal planning policies with integrated security and privacy architectures, supported by enterprise-wide contingency planning covering business continuity and disaster recovery scenarios. System and communications protection measures include network segmentation, encrypted communications protocols, and comprehensive boundary protection mechanisms across all cloud and on-premises infrastructure. Media protection policies govern the entire data lifecycle from creation to destruction, with specialized procedures for handling sensitive financial data and personally identifiable information. The company maintains robust system and information integrity controls including malware protection, vulnerability management programs, and automated patch management systems across all technology platforms. Finally, their program management framework ensures coordination between all business units, with regular executive oversight and continuous monitoring of security posture effectiveness.\",\n",
      "    \"is-compliant\": true,\n",
      "    \"non-compliant-reason\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-2\",\n",
      "    \"scenario-detail\": \"Global Financial Services Corporation, a Fortune 100 financial institution with 25,000 employees operating in 40 countries, has implemented an extensive cybersecurity and privacy governance structure following a major organizational transformation. The company's Chief Information Security Officer (CISO), working in conjunction with the Chief Privacy Officer (CPO) and enterprise risk management team, developed comprehensive organization-level policies addressing access control, audit accountability, and assessment authorization monitoring across all business units and subsidiaries. The institution established sophisticated supply chain risk management protocols requiring thorough evaluation of all technology vendors, financial service providers, and third-party contractors, with mandatory security assessments and ongoing monitoring of supplier compliance with regulatory requirements including Basel III, Dodd-Frank, and international banking regulations. Their personnel security framework encompasses detailed background investigation procedures for all staff members with access to customer financial data, including enhanced screening for positions involving payment processing systems and trading platforms. The organization implemented comprehensive incident response capabilities with specialized teams for different threat scenarios, including cyber attacks on trading systems, data breaches involving customer information, and operational disruptions affecting critical financial services. Risk assessment procedures cover enterprise-wide evaluations of market risk, operational risk, and cybersecurity threats, with quarterly assessments conducted by independent third-party security firms and internal audit teams. The company maintains robust configuration management processes for all trading systems, customer databases, and regulatory reporting platforms, with strict change control procedures requiring multiple approvals and comprehensive testing before implementation. Their awareness and training programs include specialized curricula for different roles, from basic security awareness for all employees to advanced threat hunting techniques for security analysts and compliance training for regulatory reporting staff. Physical and environmental protection measures encompass all trading floors, data centers, and customer service locations, with sophisticated access control systems, environmental monitoring, and comprehensive surveillance capabilities. Planning policies integrate cybersecurity considerations into all business processes, including new product development, merger and acquisition activities, and regulatory compliance initiatives. The institution established comprehensive contingency planning covering various disaster scenarios, from natural disasters affecting physical locations to cyber attacks targeting critical financial infrastructure. System and communications protection includes advanced network security architectures, encrypted communications for all customer transactions, and comprehensive monitoring of all data flows between internal systems and external financial networks. Media protection policies govern the handling of all financial records, customer data, and regulatory documentation throughout their lifecycle, with specialized procedures for secure destruction of sensitive materials. System and information integrity controls include real-time monitoring of all financial transactions, automated fraud detection systems, and comprehensive vulnerability management programs covering all technology platforms. The organization's program management framework ensures coordination between cybersecurity initiatives and business objectives, with regular reporting to the board of directors and regulatory agencies on security posture and compliance status.\",\n",
      "    \"is-compliant\": true,\n",
      "    \"non-compliant-reason\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-3\",\n",
      "    \"scenario-detail\": \"TechCorp's Chief Information Security Officer (CISO) Sarah Chen initiated a comprehensive enterprise-wide security assessment program following the company's recent acquisition of CloudVault Systems. The assessment covered all 15 business units across their global operations, including their primary data centers in Virginia and California, as well as their emerging IoT manufacturing division in Austin. Sarah worked closely with the Chief Privacy Officer Michael Rodriguez to develop detailed assessment procedures that addressed both security and privacy requirements across their multi-cloud infrastructure spanning AWS, Azure, and Google Cloud Platform. The assessment team, led by Senior Security Architect Jennifer Walsh, conducted thorough evaluations of access control mechanisms, focusing particularly on privileged account management across their hybrid environment. They implemented automated account monitoring systems that tracked usage patterns for over 12,000 employee accounts, including contractors and third-party vendors. The team established clear baseline configurations for all critical systems, ensuring consistent security postures across their diverse technology stack that included legacy mainframe systems, modern containerized applications, and edge computing devices. Risk assessment procedures were standardized across all business units, with quarterly reviews conducted by designated risk managers in each division. The supply chain risk management program was enhanced to address newly acquired vendor relationships from the CloudVault acquisition, implementing comprehensive vendor assessment procedures and continuous monitoring protocols. Configuration management processes were updated to include automated change control mechanisms with mandatory approval workflows for production systems. The incident response team developed new playbooks specifically addressing multi-cloud security incidents, with clear escalation procedures and communication protocols. Physical security controls were evaluated at all facility locations, including the new Austin manufacturing plant, with particular attention to environmental monitoring systems and access control mechanisms. Personnel security procedures were updated to accommodate the integration of CloudVault employees, including new background check requirements and security awareness training programs. Media protection policies were enhanced to address the increased use of removable storage devices in the manufacturing environment. The assessment also covered their expanding remote workforce of over 3,000 employees, ensuring that alternate work site security controls met enterprise standards. System and information integrity monitoring was implemented across all platforms, with centralized logging and real-time threat detection capabilities. The program management office established new governance structures to oversee the integrated security program, with regular reporting to executive leadership and board-level oversight committees.\",\n",
      "    \"is-compliant\": false,\n",
      "    \"non-compliant-reason\": \"The scenario violates the Assessment, Authorization, and Monitoring policy requirement CA-2.a which mandates that control assessments must be conducted by independent assessors or assessment teams with an adequate level of independence. The scenario describes the assessment being conducted by internal personnel (Senior Security Architect Jennifer Walsh and internal assessment teams) without any mention of independent assessors or external validation, which compromises the objectivity and credibility of the assessment results.\"\n",
      "  },\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-4\",\n",
      "    \"scenario-detail\": \"Global Financial Services Inc. embarked on a major digital transformation initiative led by Chief Technology Officer David Kim and Chief Information Security Officer Maria Santos. The initiative involved migrating their core banking systems from on-premises infrastructure to a hybrid cloud environment while maintaining compliance with SOX, PCI-DSS, and FISMA requirements. The project encompassed 22 regional offices across North America and Europe, serving over 2.5 million customers with assets exceeding $50 billion. The security team, headed by Senior Security Manager Robert Chen, developed comprehensive policies addressing all aspects of the transformation. Access control mechanisms were redesigned to support federated identity management across multiple cloud providers, implementing role-based access controls with fine-grained permissions for over 8,500 employees and contractors. The awareness and training program was expanded to include specialized modules for cloud security, with mandatory quarterly training sessions for all IT personnel and annual security awareness training for all employees. Audit and accountability measures were enhanced with centralized log management systems capable of processing over 10 terabytes of log data daily from across their distributed infrastructure. The assessment and authorization program established continuous monitoring capabilities with automated compliance reporting to regulatory bodies. Configuration management processes were updated to support infrastructure-as-code deployments with automated baseline verification and drift detection. Contingency planning procedures were redesigned to address multi-region failover scenarios, including detailed recovery procedures for critical banking applications. Identity and authentication systems were upgraded to support multi-factor authentication for all user accounts, with biometric authentication for high-privilege users. Incident response capabilities were enhanced with 24/7 security operations center monitoring and automated threat response systems. Maintenance procedures for cloud infrastructure included scheduled patching windows and emergency change procedures for critical security updates. Media protection policies addressed the secure disposal of legacy hardware and the encryption of data in transit and at rest. Physical and environmental protection measures were implemented at co-location facilities and cloud provider data centers through contractual agreements. Planning processes included detailed security and privacy plans for each system component, with regular updates reflecting changes in the threat landscape. Personnel security procedures were updated to include cloud-specific background check requirements and specialized role definitions. Risk assessment procedures incorporated cloud-specific threats and vulnerabilities, with quarterly assessments conducted by certified risk professionals. The system acquisition process included enhanced vendor security assessments and contractual security requirements for cloud service providers. System and communications protection measures included network segmentation, intrusion detection systems, and encrypted communications channels. System integrity monitoring was implemented with file integrity monitoring, malware detection, and vulnerability scanning across all platforms. Supply chain risk management procedures addressed the security of cloud service providers and third-party integrations.\",\n",
      "    \"is-compliant\": false,\n",
      "    \"non-compliant-reason\": \"The scenario violates the Personnel Security policy PS-3.a which requires personnel screening procedures to be completed before granting access to the system. The scenario mentions updating personnel security procedures to include 'cloud-specific background check requirements' as part of the transformation initiative, but does not indicate that proper screening was completed before personnel were granted access to the new cloud systems during the migration process. This suggests that employees may have been given access to cloud systems before completing the required screening procedures.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Generate 4 scenarios in 2 batches of 2 each\n",
    "# Batch 0 (even): compliant scenarios with IDs scenario-id-1, scenario-id-2\n",
    "# Batch 1 (odd): non-compliant scenarios with IDs scenario-id-3, scenario-id-4\n",
    "scenarios = generate_compliance_scenarios(\n",
    "    knowledge_base_id=KNOWLEDGE_BASE_ID,\n",
    "    model_arn=MODELS['balanced'],\n",
    "    batch_size=2,\n",
    "    total_scenarios=4\n",
    ")\n",
    "save_scenarios_to_file(scenarios, '/home/sagemaker-user/scenarios.json')\n",
    "save_scenarios_to_s3(scenarios, OUTPUT_BUCKET, OUTPUT_PREFIX, \"scenarios.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "a3c8d612-ad8e-4bc2-b653-ac967d5e712e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22d566-aedd-4ed6-bcad-42f9aad14284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
