{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f82c405-65c9-499e-91cb-c595d8b1e773",
   "metadata": {},
   "source": [
    "Using Generated Scenarios for RAG Evaluation\n",
    "Once you have your scenarios, you can use them as a prompt dataset for RAG evaluation. Amazon Bedrock Knowledge Bases RAG evaluation allows you to evaluate your retrieval-augmented generation applications, with metrics such as correctness, completeness, and faithfulness (hallucination detection) [3]. To evaluate retrieval and generation for an Amazon Bedrock Knowledge Base, you provide a prompt dataset stored in Amazon S3 using the JSON Lines format with a .jsonl file extension [4]. Each line must be a valid JSON object, and there can be up to 1000 prompts in your dataset per evaluation job [4].\n",
    "\n",
    "Converting Scenarios to Evaluation Dataset\n",
    "def convert_to_evaluation_dataset(scenarios: List[Dict], output_path: str):\n",
    "    \"\"\"\n",
    "    Convert generated scenarios to JSONL format for RAG evaluation.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        for scenario in scenarios:\n",
    "            eval_entry = {\n",
    "                \"prompt\": scenario['scenario_description'],\n",
    "                \"referenceResponse\": json.dumps({\n",
    "                    \"evaluation_result\": scenario['evaluation_result'],\n",
    "                    \"reasoning\": scenario['evaluation_reasoning'],\n",
    "                    \"violated_policies\": scenario['violated_policies']\n",
    "                }),\n",
    "                \"category\": \"compliant\" if scenario['evaluation_result'] else \"non_compliant\"\n",
    "            }\n",
    "            f.write(json.dumps(eval_entry) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "Key Considerations\n",
    "When generating synthetic data for compliance evaluation, organizations must typically secure End User License Agreements and might need access to multiple LLMs [1]. Although the process demands minimal human expert validation, these strategic requirements underscore the complexity of generating synthetic datasets efficiently [1].\n",
    "\n",
    "For quality assurance, consider using the LLM-as-a-judge framework available in Amazon Bedrock evaluations, which can assess quality metrics such as correctness, completeness, and faithfulness [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dee5a3-7b80-455e-aa3e-89da15c29df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
