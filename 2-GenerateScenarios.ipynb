{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c22ff17-b692-4612-b2ce-785abdec9713",
   "metadata": {},
   "source": [
    "# GenerateScenarios\n",
    "This notebook generates realistic compliance scenarios by leveraging AWS Bedrock's Retrieval-Augmented Generation (RAG) capabilities using the NIST control framework and specific organizational policies generated against that framework. It generates 1,000 (500 compliant, 500 non-compliant) complex, multi-policy scenarios that simulate real-world compliance situations (like employee onboarding, data access requests, or security incidents).\n",
    "\n",
    " - Generates Scenario Batches - Uses Bedrock's Converse API with RAG to create realistic compliance scenarios by alternating between compliant and non-compliant cases.  For each batch of scenarios, it retrieves policy context by querying AWS Bedrock Knowledge Base containing NIST controls and policies.\n",
    " - Structures Output - Forces JSON format output with specific schema including scenario ID, detailed description, compliance status, and violation reasons.\n",
    " - Multi-Policy Coverage - Each scenario incorporates multiple policies to create complex, realistic compliance evaluation situations.\n",
    "\n",
    "```\n",
    "Execution Flow:\n",
    "└── main()\n",
    "    ├── generate_compliance_scenarios()\n",
    "    │   ├── retrieve_kb_context()\n",
    "    │   │   └── bedrock_agent_runtime.retrieve()\n",
    "    │   └── generate_scenario_batch()\n",
    "    │       └── bedrock_runtime.converse()\n",
    "    ├── save_scenarios_to_file()\n",
    "    └── save_scenarios_to_s3()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "6b008304-8133-4539-b75f-b66534b07807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:56:12.486228Z",
     "iopub.status.busy": "2026-01-11T23:56:12.485783Z",
     "iopub.status.idle": "2026-01-11T23:56:12.599815Z",
     "shell.execute_reply": "2026-01-11T23:56:12.598437Z",
     "shell.execute_reply.started": "2026-01-11T23:56:12.486199Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "BUCKET = '183023889407-us-east-1-compliance-rule-generator'\n",
    "S3_PREFIX_POLICY_MARKDOWN_ALL = 'policies/markdown/all-policies-main/'\n",
    "OUTPUT_PREFIX = 'scenarios/'  # Folder path for results\n",
    "KNOWLEDGE_BASE_ID = 'T8EW10IU3Z'  # AWS Bedrock Knowledge Base containing NIST policies\n",
    "# Available Bedrock model ARNs with performance notes\n",
    "MODELS = {\n",
    "    'premium': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/global.anthropic.claude-opus-4-5-20251101-v1:0', # not available\n",
    "    'good': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/global.anthropic.claude-sonnet-4-5-20250929-v1:0', # times out\n",
    "    'balanced': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0',  # recommended\n",
    "    'fast_cheap': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-haiku-4-5-20251001-v1:0',\n",
    "    'aws_native_premier': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-premier-v1:0',\n",
    "    'aws_native_pro': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-pro-v1:0'\n",
    "}\n",
    "MODEL_ARN = MODELS['good']  # Default model selection\n",
    "SCENARIOS_PER_BATCH = 2\n",
    "NUM_BATCHES = 4\n",
    "TOTAL_SCENARIOS = SCENARIOS_PER_BATCH * NUM_BATCHES\n",
    "POLICIES_PER_SCENARIO = 4\n",
    "\n",
    "# JSON tool configuration for Bedrock Converse API\n",
    "# Forces the model to return structured JSON with specific schema\n",
    "TOOL_CONFIG = {\n",
    "    \"tools\": [{\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"scenario_json\",\n",
    "            \"description\": \"Return compliance scenarios as JSON\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"scenarios\": {  # Array of scenario objects\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"scenario-id\": {\"type\": \"string\"},      # Format: scenario-id-1, scenario-id-2, etc.\n",
    "                                    \"scenario-detail\": {\"type\": \"string\"},  # Detailed scenario description (200+ words)\n",
    "                                    \"is-compliant\": {\"type\": \"boolean\"},     # True if compliant, False if non-compliant\n",
    "                                    \"non-compliant-reason\": {\"type\": \"string\"}  # If non-compliant, why? Which policy(s) were violated?\n",
    "                                },\n",
    "                                \"required\": [\"scenario-id\", \"scenario-detail\", \"is-compliant\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"scenarios\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }],\n",
    "    \"toolChoice\": {\"tool\": {\"name\": \"scenario_json\"}}  # Force use of the JSON tool\n",
    "}\n",
    "\n",
    "# Initialize AWS Bedrock clients\n",
    "bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name='us-east-1')  # For knowledge base retrieval\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')  # For model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "6bdc29ec-8ace-4cca-abb9-0139668e1bb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:56:12.885104Z",
     "iopub.status.busy": "2026-01-11T23:56:12.884576Z",
     "iopub.status.idle": "2026-01-11T23:56:12.896613Z",
     "shell.execute_reply": "2026-01-11T23:56:12.894750Z",
     "shell.execute_reply.started": "2026-01-11T23:56:12.885062Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_scenario_batch(\n",
    "    policies: str,\n",
    "    policy_ids: str,\n",
    "    batch_num: int,\n",
    "    model_arn: str,\n",
    "    scenarios_per_batch: int=SCENARIOS_PER_BATCH\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate one batch of compliance scenarios using Bedrock Converse API.\n",
    "    This function creates realistic compliance scenarios by alternating between\n",
    "    compliant and non-compliant scenarios across batches. Uses the JSON tool\n",
    "    to ensure structured output.\n",
    "    Args:\n",
    "        policies: Combined policy text from knowledge base\n",
    "        batch_num: Current batch number (0-indexed)\n",
    "        model_arn: AWS Bedrock model ARN or ID\n",
    "        scenarios_per_batch: Number of scenarios per batch\n",
    "    Returns:\n",
    "        List of scenario dictionaries with keys: scenario-id, scenario-detail, is-compliant, non-compliant-reason\n",
    "    \"\"\"\n",
    "    # Alternate between compliant (even batches) and non-compliant (odd batches) scenarios\n",
    "    is_compliant = batch_num % 2 == 0\n",
    "    \n",
    "    # Calculate starting ID for consecutive numbering across all batches\n",
    "    start_id = batch_num * scenarios_per_batch + 1\n",
    "    \n",
    "    # Construct detailed prompt with all policy context and specific requirements\n",
    "    prompt = f\"\"\"Based on NIST compliance policies under **KNOWLEDGE BASE CONTEXT**, generate {scenarios_per_batch} realistic compliance scenario (scenario-details) \n",
    "    that are \n",
    "    {'compliant' if is_compliant else 'non-compliant.  Ensure that only one underlying policy is violated out of all the underlying policies for each scenario. Use no leading language like \"however\" or tone to indicate in any way that the scenarios is in any way invalid.  Try to trick the reader to believe it is valid.'}.\n",
    "    \n",
    "    Each scenario-detail must:\n",
    "    - Use all policies from KNOWLEDGE BASE CONTEXT context below\n",
    "    - Include specific business details (roles, systems, data, actions)\n",
    "    - Be realistic and detailed (200+ words)\n",
    "    - Have scenario-id format: scenario-id-{start_id}, scenario-id-{start_id+1}, etc.\n",
    "\n",
    "    **Avoid generating scenarios based on cost-benefit principles or concentration percentages.\n",
    "    \n",
    "    **Note that non-US citizens cannot obtain US security clearances.**\n",
    "    \n",
    "    Return as JSON array with fields: scenario-id, scenario-detail, is-compliant, non-compliant-reason\n",
    "    {'leave non-compliant-reason empty' if is_compliant else 'For non-compliant-reason, provide an explanation of exactly why the scenario is non-compliant, including which policy(s) were violated.'}\n",
    "\n",
    "    **Here is an example of scenario-detail:**\n",
    "    SecureDefense Corporation, a defense contractor with 8,500+ employees supporting classified government projects, is deploying a new secure development environment\n",
    "    for processing TOP SECRET/SCI information across 12 secure facilities. The Chief Security Officer implemented citizenship requirements per MA-5.3 ensuring all\n",
    "    personnel performing maintenance on classified systems are verified U.S. citizens with current documentation including birth certificates, passports, and security\n",
    "    clearance validation, with emergency maintenance procedures explicitly prohibiting non-citizen access under any circumstances. The organization deployed NIAP-approved\n",
    "    protection profiles per SA-4.7 for all commercial security products including firewalls evaluated against Network Device Protection Profile v2.2, VPN solutions meeting\n",
    "    IPsec Virtual Private Network (VPN) Client Protection Profile v1.0, and database management systems validated against Database Management System Protection Profile v4.0,\n",
    "    with FIPS 140-2 Level 3 validated cryptographic modules for products without applicable NIAP profiles. The system architecture team implemented hierarchical protection\n",
    "    per SA-8.12 with hypervisor components at highest trust level protecting against guest VMs, classified application containers at medium trust level isolated from\n",
    "    development tools, and development interfaces at lowest trust level with comprehensive access controls preventing privilege escalation across trust boundaries. The\n",
    "    procurement team maintained technology diversity per SC-29 across the secure development pipeline using Red Hat and SUSE Linux distributions (45% and 35% respectively),\n",
    "    Dell and HPE hardware platforms, and Raytheon and General Dynamics security appliances to minimize supply chain concentration risks. Cost-benefit analyses per SA-8.25\n",
    "    documented security investments of $4.2M annually justified by protecting classified intellectual property valued at $2.8B and avoiding potential contract termination\n",
    "    penalties exceeding $150M for security violations.\n",
    "\n",
    "    **KNOWLEDGE BASE CONTEXT**\n",
    "    {policies}\n",
    "    \"\"\"\n",
    "    # Extract model ID from ARN (Converse API requires model ID, not full ARN)\n",
    "    model_id = model_arn.split('/')[-1] if '/' in model_arn else model_arn\n",
    "    \n",
    "    # Call Bedrock Converse API with JSON tool enforcement\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "        toolConfig=TOOL_CONFIG,  # Forces structured JSON output\n",
    "        inferenceConfig={\"maxTokens\": 4096, \"temperature\": 0.7}  # Allow creative but controlled generation, about 3,000 words max\n",
    "    )\n",
    "    \n",
    "    # Extract scenarios from tool use response\n",
    "    if response.get('stopReason') == 'tool_use':\n",
    "        for content_block in response['output']['message']['content']:\n",
    "            if 'toolUse' in content_block:\n",
    "                scenarios_data = content_block['toolUse']['input']\n",
    "                scenarios = scenarios_data.get('scenarios', [])\n",
    "                for scenario in scenarios:\n",
    "                    scenario['scenario-detail'] += f\"\\n\\nPolicies referenced: {policy_ids}\"\n",
    "                return scenarios\n",
    "    \n",
    "    # Return empty list if no tool use or scenarios found\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "57b41f28-b131-413b-be13-370ed0246637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:56:13.344236Z",
     "iopub.status.busy": "2026-01-11T23:56:13.343832Z",
     "iopub.status.idle": "2026-01-11T23:56:13.358372Z",
     "shell.execute_reply": "2026-01-11T23:56:13.356138Z",
     "shell.execute_reply.started": "2026-01-11T23:56:13.344178Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_s3_policies(bucket: str = BUCKET, policies_per_scenario: int = POLICIES_PER_SCENARIO) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Retrieve random policy documents from S3 bucket.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # List all policy files\n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix=S3_PREFIX_POLICY_MARKDOWN_ALL\n",
    "    )\n",
    "    \n",
    "    policy_files = [obj['Key'] for obj in response.get('Contents', []) \n",
    "                   if obj['Key'].endswith('.md')]\n",
    "    \n",
    "    # Randomly select needed number of policies\n",
    "    selected_files = random.sample(policy_files, min(policies_per_scenario, len(policy_files)))\n",
    "    \n",
    "    # Read selected policy contents\n",
    "    policies = []\n",
    "    policy_ids = []\n",
    "    for file_key in selected_files:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=file_key)\n",
    "        content = obj['Body'].read().decode('utf-8')\n",
    "        policies.append(content)\n",
    "        policy_ids.append(\"policy_\" + re.search(r\"POLICY: ([A-Z]+-[\\d.]+)\", content).group(1))\n",
    "            \n",
    "    print(f\"Retrieved {len(policies)} random policies from S3\")\n",
    "    return '\\n\\n'.join(policies), ', '.join(policy_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "11aec7fa-498b-436f-8b4e-30961c28bc88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:56:13.876711Z",
     "iopub.status.busy": "2026-01-11T23:56:13.876435Z",
     "iopub.status.idle": "2026-01-11T23:56:13.888568Z",
     "shell.execute_reply": "2026-01-11T23:56:13.885355Z",
     "shell.execute_reply.started": "2026-01-11T23:56:13.876688Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_compliance_scenarios(\n",
    "    knowledge_base_id: str = KNOWLEDGE_BASE_ID,\n",
    "    model_arn: str = MODEL_ARN,\n",
    "    scenarios_per_batch: int=SCENARIOS_PER_BATCH,\n",
    "    policies_per_scenario: int=POLICIES_PER_SCENARIO,\n",
    "    total_scenarios: int = TOTAL_SCENARIOS\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Main orchestrator function for generating compliance scenarios.\n",
    "    \n",
    "    This function coordinates the entire scenario generation process:\n",
    "    1. Retrieves comprehensive policy context from knowledge base (once)\n",
    "    2. Generates scenarios in batches to manage API limits and costs\n",
    "    3. Alternates between compliant and non-compliant scenarios\n",
    "    4. Implements rate limiting to avoid API throttling\n",
    "    Args:\n",
    "        knowledge_base_id: AWS Bedrock Knowledge Base ID\n",
    "        model_arn: AWS Bedrock model ARN to use for generation\n",
    "        scenarios_per_batch: Number of scenarios per batch\n",
    "        policies_per_scenario: Number of NIST policies to use in creating a scenario\n",
    "        total_scenarios: Total number of scenarios to generate\n",
    "    Returns:\n",
    "        List of all generated scenario dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate scenarios in batches\n",
    "    all_scenarios = []\n",
    "    for batch_num in range(total_scenarios // scenarios_per_batch):\n",
    "      # Retrieve all policy context once (reused across all batches)\n",
    "        try:\n",
    "            policies, policy_ids = retrieve_s3_policies(BUCKET, policies_per_scenario)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving from KB: {e}\")\n",
    "            return []\n",
    "        try:\n",
    "            # Generate one batch of scenarios          \n",
    "            scenarios = generate_scenario_batch(policies, policy_ids, batch_num, model_arn, scenarios_per_batch)\n",
    "            all_scenarios.extend(scenarios)\n",
    "            print(f\"Batch {batch_num + 1}: Generated {len(scenarios)} scenarios\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating scenario batch {batch_num + 1}: {e}\")\n",
    "            continue  # excplicit to go to next scenario, maybe this was a temporary glitch\n",
    "        \n",
    "        # Rate limiting: pause between batches to avoid API throttling\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return all_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_state": "idle",
   "id": "8712a78b-70a0-4f46-9fbc-0b8b6e5a16cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:56:14.516952Z",
     "iopub.status.busy": "2026-01-11T23:56:14.516548Z",
     "iopub.status.idle": "2026-01-11T23:56:14.526687Z",
     "shell.execute_reply": "2026-01-11T23:56:14.524916Z",
     "shell.execute_reply.started": "2026-01-11T23:56:14.516923Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_scenarios_to_file(scenarios: List[Dict], output_path: str):\n",
    "    \"\"\"\n",
    "    Save generated scenarios to a JSON file with metadata.\n",
    "    \n",
    "    Creates a structured JSON file containing:\n",
    "    - Summary statistics (total, compliant, non-compliant counts)\n",
    "    - All generated scenarios\n",
    "    \"\"\"\n",
    "    # Print scenarios to console for immediate review\n",
    "    print(json.dumps(scenarios, indent=2))\n",
    "    \n",
    "    # Save to file with metadata and statistics\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'total_scenarios': len(scenarios),\n",
    "            'compliant_count': sum(1 for s in scenarios if s['is-compliant']),\n",
    "            'non_compliant_count': sum(1 for s in scenarios if not s['is-compliant']),\n",
    "            'scenarios': scenarios\n",
    "        }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_state": "idle",
   "id": "2149410b-8800-4e63-83f1-9b9cda20743c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:56:15.081408Z",
     "iopub.status.busy": "2026-01-11T23:56:15.081133Z",
     "iopub.status.idle": "2026-01-11T23:56:15.089682Z",
     "shell.execute_reply": "2026-01-11T23:56:15.087215Z",
     "shell.execute_reply.started": "2026-01-11T23:56:15.081386Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_scenarios_to_s3(scenarios: List[Dict], output_bucket: str = BUCKET, output_prefix: str = OUTPUT_PREFIX, object_name: str = \"scenarios.json\"):\n",
    "    \"\"\"\n",
    "    Save generated scenarios to a S3.\n",
    "\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    json_data = json.dumps({\"scenarios\": scenarios}, indent=2)\n",
    "    s3.put_object(Bucket=output_bucket, Key=output_prefix+object_name, Body=json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "execution_state": "idle",
   "id": "034d84a6-9c5f-446e-a1ce-5912d269b599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:56:15.973513Z",
     "iopub.status.busy": "2026-01-11T23:56:15.972762Z",
     "iopub.status.idle": "2026-01-11T23:56:16.006934Z",
     "shell.execute_reply": "2026-01-11T23:56:15.998228Z",
     "shell.execute_reply.started": "2026-01-11T23:56:15.973068Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Example usage: Generate 4 scenarios in 2 batches of 2 each\n",
    "    # Batch 0 (even): compliant scenarios with IDs scenario-id-1, scenario-id-2\n",
    "    # Batch 1 (odd): non-compliant scenarios with IDs scenario-id-3, scenario-id-4\n",
    "    scenarios = generate_compliance_scenarios(\n",
    "        knowledge_base_id=KNOWLEDGE_BASE_ID,\n",
    "        model_arn=MODELS['balanced'],\n",
    "        scenarios_per_batch=SCENARIOS_PER_BATCH,\n",
    "        policies_per_scenario=POLICIES_PER_SCENARIO,\n",
    "        total_scenarios=TOTAL_SCENARIOS \n",
    "    )\n",
    "\n",
    "    save_scenarios_to_file(scenarios, '/home/sagemaker-user/scenarios.json')\n",
    "    save_scenarios_to_s3(scenarios, BUCKET, OUTPUT_PREFIX, \"scenarios.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "execution_state": "idle",
   "id": "a3c8d612-ad8e-4bc2-b653-ac967d5e712e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T23:56:18.509436Z",
     "iopub.status.busy": "2026-01-11T23:56:18.508949Z",
     "iopub.status.idle": "2026-01-11T23:58:00.413628Z",
     "shell.execute_reply": "2026-01-11T23:58:00.412344Z",
     "shell.execute_reply.started": "2026-01-11T23:56:18.509405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 random policies from S3\n",
      "Batch 1: Generated 2 scenarios\n",
      "Retrieved 4 random policies from S3\n",
      "Batch 2: Generated 2 scenarios\n",
      "Retrieved 4 random policies from S3\n",
      "Batch 3: Generated 2 scenarios\n",
      "Retrieved 4 random policies from S3\n",
      "Batch 4: Generated 2 scenarios\n",
      "[\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-1\",\n",
      "    \"scenario-detail\": \"TechGuard Financial Services, a major financial institution with 12,000+ employees processing customer PII across 45 branch locations, implemented a comprehensive privacy and security framework for their new mobile banking platform serving 2.8 million customers. The Chief Privacy Officer established rigorous PII disclosure accounting per PM-21, requiring all customer data disclosures to regulatory agencies, credit bureaus, and third-party processors be recorded within 24 hours in their automated Privacy Management System, with detailed records including disclosure date, specific data elements shared (account balances, transaction history, personal identifiers), business purpose (regulatory compliance, credit verification, fraud prevention), and complete recipient contact information including designated privacy officers at each receiving organization. The system maintains comprehensive audit trails of all record access and modifications, with disclosure accounting records retained for seven years (exceeding the five-year minimum) and automated responses to customer requests for disclosure accounting delivered within 15 days. The Business Continuity Manager conducted extensive capacity planning per CP-2.2, documenting minimum processing requirements of 50,000 concurrent users during contingency operations (reduced from normal 150,000 capacity), telecommunications bandwidth requirements of 2.5 Gbps for essential banking services during degraded operations, and environmental support specifications including backup power systems capable of supporting 72 hours of operations across primary and alternate data centers. Annual capacity validation testing confirmed the ability to maintain core banking functions including account access, bill payments, and fraud monitoring during simulated disaster scenarios. The organization deployed comprehensive sensor usage controls per SC-42.2 for their mobile app location services and biometric authentication systems, with documented authorized uses limited to fraud prevention, regulatory compliance, and customer-requested services, supported by contractual restrictions with third-party analytics providers explicitly prohibiting customer behavior profiling or marketing data sales. Development teams maintained strict integrity mapping per SA-10.5 between master build repositories and production deployments, with automated cryptographic hash verification performed within 12 hours of any security-relevant component updates, comprehensive documentation of all version control activities, and immediate resolution protocols for any mapping discrepancies affecting the mobile banking platform's security functions.\\n\\nPolicies referenced: policy_PM-21, policy_CP-2.2, policy_SC-42.2, policy_SA-10.5\",\n",
      "    \"is-compliant\": true,\n",
      "    \"non-compliant-reason\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-2\",\n",
      "    \"scenario-detail\": \"MedSecure Healthcare Network, a multi-state healthcare provider with 6,800 employees managing patient records across 28 medical facilities, deployed an integrated patient monitoring and electronic health record system processing sensitive medical information for 450,000+ patients. The Data Protection Officer implemented comprehensive PII disclosure accounting per PM-21 for all patient data sharing with insurance providers, specialist physicians, and medical research institutions, utilizing an automated Health Information Exchange tracking system that records every disclosure within 8 hours including specific medical data categories shared (diagnostic results, treatment histories, prescription records), disclosure purposes (treatment coordination, insurance claims processing, approved research studies), complete recipient healthcare provider information with NPI numbers and HIPAA compliance certifications, and maintains audit trails accessible to patients within 20 days of valid requests. All disclosure records are retained for ten years per healthcare regulations, exceeding the five-year minimum requirement. The Business Continuity Manager established detailed capacity planning per CP-2.2 addressing critical patient care systems, with documented minimum processing capacity of 15,000 concurrent electronic health record users during emergency operations (reduced from normal 35,000), telecommunications requirements ensuring 1.8 Gbps bandwidth for telemedicine and emergency communications during degraded operations, and environmental support planning including medical-grade power systems with 96-hour backup capacity and specialized cooling systems for medical equipment across primary and disaster recovery sites. Quarterly capacity testing validates the ability to maintain life-critical patient monitoring, emergency department operations, and pharmaceutical management during contingency scenarios. The organization deployed strict sensor authorization controls per SC-42.2 for patient monitoring devices, security cameras in restricted areas, and mobile health applications, with authorized uses explicitly limited to patient care, facility security, and regulatory compliance, supported by vendor contracts containing specific restrictions prohibiting patient data use for commercial purposes or unauthorized research. The development team maintains rigorous version control integrity mapping per SA-10.5 for all medical device software and security components, performing automated SHA-256 hash verification within 6 hours of any updates to life-critical systems, maintaining detailed change documentation with medical device compliance tracking, and implementing immediate escalation procedures for any integrity discrepancies affecting patient safety systems or PHI security controls.\\n\\nPolicies referenced: policy_PM-21, policy_CP-2.2, policy_SC-42.2, policy_SA-10.5\",\n",
      "    \"is-compliant\": true,\n",
      "    \"non-compliant-reason\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-3\",\n",
      "    \"scenario-detail\": \"TechnoSecure Industries, a leading cybersecurity firm with 3,200 employees, is conducting comprehensive control assessments for their flagship cloud security platform serving Fortune 500 clients. The Chief Information Security Officer has established a robust assessment program with certified control assessors possessing CISSP and CISA credentials with 5+ years of experience conducting independent assessments. The assessment team developed detailed assessment plans documenting scope covering 247 security controls, standardized testing procedures aligned with NIST SP 800-53A, comprehensive environment analysis including cloud and hybrid components, qualified team composition with specialized expertise areas, and clear roles and responsibilities matrix approved by the Authorizing Official within required timeframes. The organization maintains strict assessment frequencies with high-impact systems assessed annually, moderate-impact systems every 18 months, and low-impact systems every 3 years, with continuous monitoring capabilities providing real-time control effectiveness validation. Assessment reports document detailed findings with control implementation status, operational effectiveness ratings, and compliance determinations distributed to system owners, security teams, and executive leadership within 15 days of assessment completion. The audit record review program operates with Security Operations Center analysts performing daily reviews of high-impact system logs, weekly analysis of moderate-impact systems, and monthly reviews for low-impact systems, utilizing SIEM correlation rules to identify failed authentication attempts, privilege escalation events, unauthorized configuration changes, and anomalous data access patterns. When the organization's threat intelligence team received credible information about advanced persistent threats targeting similar organizations, the audit review frequency was immediately increased for all internet-facing systems and notification procedures were enhanced to ensure security findings are escalated to incident response teams within 12 hours. All developers contracted for system components provide comprehensive functional property descriptions focusing on externally visible security control interfaces, API specifications, and user-accessible functionality while properly excluding internal implementation algorithms and proprietary code structures, with all documentation reviewed and approved by system owners and the CISO before deployment authorization. The organization's privacy program ensures that when personally identifiable information requires correction or deletion, automated notification systems immediately alert all affected individuals within 8 business days and notify authorized recipients including data processors, business partners, and regulatory agencies within 3 business days, maintaining comprehensive notification records for 5 years to demonstrate compliance with privacy requirements.\\n\\nPolicies referenced: policy_SI-18.5, policy_SA-4.1, policy_AU-6, policy_CA-2\",\n",
      "    \"is-compliant\": false,\n",
      "    \"non-compliant-reason\": \"This scenario violates AU-6 RULE-01 which requires high-impact systems to have audit records reviewed at frequencies not exceeding 7 days. The scenario states that high-impact systems receive daily reviews, but when threat intelligence was received about advanced persistent threats, the audit review frequency was 'immediately increased for all internet-facing systems' rather than being adjusted within the required 30-day timeframe as specified in RULE-04. While the scenario appears compliant on the surface with daily reviews, the failure to formally adjust the documented review procedures within 30 days of receiving credible threat intelligence constitutes a violation.\"\n",
      "  },\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-4\",\n",
      "    \"scenario-detail\": \"DataGuard Corporation, a healthcare technology company with 4,800 employees managing electronic health records for 2.3 million patients across 450 medical facilities, has implemented comprehensive compliance programs across their HIPAA-regulated infrastructure. The organization's control assessment program utilizes certified assessors with appropriate independence levels, including external third-party assessors for initial authorizations and qualified internal assessors for ongoing monitoring activities, all possessing healthcare cybersecurity certifications and 7+ years of relevant experience. Assessment plans are meticulously developed including detailed scope definitions covering all 324 implemented security and privacy controls, standardized assessment procedures based on NIST SP 800-53A methodology, comprehensive environment documentation encompassing cloud, on-premises, and hybrid components, qualified assessment team composition with healthcare domain expertise, and clearly defined roles and responsibilities matrices. The Authorizing Official reviews and approves all assessment plans within established timeframes, with assessment execution occurring annually for high-impact systems, every 18 months for moderate-impact systems, and every 3 years for low-impact systems. Assessment reports provide detailed documentation of control implementation status, operational effectiveness determinations, and compliance findings distributed to appropriate stakeholders including system owners, privacy officers, and executive leadership within 20 days of assessment completion. The audit record review program operates through a 24/7 Security Operations Center with analysts performing real-time monitoring of critical healthcare systems, daily analysis of high-impact systems processing protected health information, weekly reviews of moderate-impact administrative systems, and monthly analysis of low-impact support systems. When personally identifiable information requires correction or deletion, the organization's automated privacy management system ensures affected individuals receive notification within 8 business days and all authorized recipients including healthcare partners, insurance providers, and regulatory bodies are notified within 4 business days, with comprehensive notification records maintained for 7 years. The organization's secure development program requires all system developers and component vendors to provide detailed functional property descriptions focusing on externally visible security control interfaces, healthcare interoperability standards compliance, user authentication mechanisms, and audit logging capabilities while appropriately excluding proprietary algorithms and internal processing logic. All functional property documentation undergoes thorough review by system owners and receives formal approval from the Chief Information Security Officer before system deployment or service activation, ensuring complete alignment with healthcare regulatory requirements and organizational security standards.\\n\\nPolicies referenced: policy_SI-18.5, policy_SA-4.1, policy_AU-6, policy_CA-2\",\n",
      "    \"is-compliant\": false,\n",
      "    \"non-compliant-reason\": \"This scenario violates SI-18.5 RULE-03 which requires all authorized recipients of corrected or deleted PII to be notified within 5 business days of the correction or deletion action. The scenario states that authorized recipients including healthcare partners, insurance providers, and regulatory bodies are notified within 4 business days, which appears compliant. However, the scenario also mentions that the organization maintains notification records for 7 years, when RULE-04 only requires notification records to be maintained for a minimum of 3 years. While maintaining records longer than required is not itself a violation, the scenario violates RULE-03 by stating notifications occur within 4 business days when the rule requires 5 business days - this creates an impossible standard that cannot be consistently met.\"\n",
      "  },\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-5\",\n",
      "    \"scenario-detail\": \"TechnoSecure Industries, a financial technology company with 3,200 employees processing sensitive customer financial data, is implementing a comprehensive security awareness program across their distributed workforce spanning 15 locations. The Chief Information Security Officer established mandatory practical exercises per AT-3.3 ensuring all security training programs include hands-on simulations with software developers completing secure coding exercises annually focusing on SQL injection prevention, cross-site scripting mitigation, and buffer overflow protection using vulnerable code laboratories and penetration testing scenarios. Senior executives and C-level leadership participate in quarterly targeted phishing simulation exercises designed to mimic advanced persistent threat campaigns, with failure rates tracked and remedial training provided within 30 days of unsuccessful attempts. Privacy personnel complete annual practical exercises on conducting privacy impact assessments using real-world data processing scenarios and PII handling simulations with performance metrics measuring identification accuracy and response time to privacy incidents. The organization obtained comprehensive system documentation per SA-5 for all critical financial processing systems including administrator documentation describing secure configuration of payment processing gateways, fraud detection algorithms, and encryption key management systems, with user documentation provided to customer service representatives detailing secure methods for accessing customer accounts and handling sensitive financial information. When third-party vendor documentation was unavailable for legacy mainframe systems, documented attempts to obtain missing materials were completed within 30 days and remediation actions implemented including reverse-engineering configuration guides and establishing internal knowledge transfer sessions. The infrastructure team implemented priority-based resource allocation per SC-6 with payment processing systems receiving highest priority allocation of CPU, memory, and network bandwidth, customer service applications at medium priority, and development environments at lowest priority with resource quotas preventing any single process from consuming more than 60% of available system resources. Audit log transfer mechanisms per AU-4.1 automatically transfer security logs from critical payment processing systems to alternate storage every 4 hours with verification checksums confirming successful delivery, while standard business applications transfer logs daily to cloud-based SIEM systems with automated alerts generated for any transfer failures within 2 hours.\\n\\nPolicies referenced: policy_AT-3.3, policy_SA-5, policy_SC-6, policy_AU-4.1\",\n",
      "    \"is-compliant\": true,\n",
      "    \"non-compliant-reason\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-6\",\n",
      "    \"scenario-detail\": \"GlobalMed Healthcare Systems, a healthcare provider network with 12,000 employees managing electronic health records across 45 medical facilities, deployed an integrated security and privacy training program addressing HIPAA compliance requirements and cybersecurity threats. The Training Manager implemented comprehensive practical exercises per AT-3.3 with all healthcare staff participating in annual hands-on simulations including phishing email identification workshops, medical device security scenarios, and patient data breach response exercises with pass/fail criteria requiring 85% accuracy rates and mandatory remedial training for failures. Software development teams building electronic health record systems complete quarterly secure coding exercises addressing healthcare-specific vulnerabilities including HL7 message injection attacks, FHIR API security flaws, and medical device communication protocols with performance metrics tracking vulnerability detection rates and secure coding implementation scores. Executive leadership including Chief Medical Officer and Chief Privacy Officer participate in monthly targeted phishing simulations using healthcare-themed social engineering attacks with immediate feedback and security awareness reinforcement. Privacy officers complete semi-annual practical exercises on HIPAA risk assessments, patient consent management scenarios, and breach notification procedures using simulated patient data incidents. The organization maintains complete system documentation per SA-5 for all medical information systems including detailed administrator documentation for electronic health record databases describing secure patient data encryption, access control configurations, and audit trail management, with user documentation provided to medical staff explaining secure login procedures, patient data access protocols, and privacy protection responsibilities. Third-party medical device documentation was systematically obtained from manufacturers with documented attempts for legacy equipment and remediation plans including vendor security assessments and internal configuration documentation. Infrastructure teams implemented priority-based resource allocation per SC-6 ensuring critical patient monitoring systems receive maximum CPU and network priority, electronic health records maintain high priority allocation, and administrative systems operate at standard priority levels with resource quotas limiting individual applications to 40% of server capacity to prevent system interference. Audit log management per AU-4.1 transfers security logs from patient monitoring systems and medical devices to HIPAA-compliant alternate storage every 2 hours with cryptographic verification, while administrative systems transfer logs every 6 hours to secure cloud storage with automated monitoring detecting transfer failures and generating alerts within 1 hour for immediate investigation by the security operations center.\\n\\nPolicies referenced: policy_AT-3.3, policy_SA-5, policy_SC-6, policy_AU-4.1\",\n",
      "    \"is-compliant\": true,\n",
      "    \"non-compliant-reason\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-7\",\n",
      "    \"scenario-detail\": \"TechnoSphere Industries, a Fortune 500 technology company with 15,000+ employees across 25 global offices, is implementing a comprehensive security modernization initiative for their enterprise infrastructure. The IT Security Manager established detailed software installation policies per CM-11 governing all workstations, servers, and mobile devices, with explicit definitions of permitted software sources including Microsoft Store for Business, Apple App Store for Business, and pre-approved vendor portals like Adobe Creative Cloud and Atlassian marketplace. Users can install security patches from official vendor sites, business-approved applications from organizational app stores, and development tools with proper approval documentation. The organization deployed automated enforcement through Microsoft System Center Configuration Manager and Group Policy Objects preventing installation of unauthorized software, with quarterly compliance monitoring generating detailed reports of all installation attempts. All software installation activities are logged through centralized SIEM systems capturing timestamps, user identities, software names, and approval status. The Information Disposal program follows SI-12.3 requirements with NIST 800-88 compliant destruction methods for high-sensitivity data including employee records, financial data, and customer PII using certified destruction services with certificates of destruction. Retention schedules are established by data type with automatic disposal triggers 30 days after retention period expiration unless legal holds apply. The cryptographic infrastructure implements SC-12.2 controls using FIPS 140-2 validated Hardware Security Modules from Thales and SafeNet for all symmetric key operations including AES-256 encryption keys for database encryption, SSL/TLS certificates, and VPN tunneling. Key generation, distribution, and lifecycle management processes are fully documented and audited quarterly. Physical security controls per PE-5.2 require badge authentication on all 847 network printers and multifunction devices across facilities, with unclaimed documents automatically purged after 4 hours for sensitive materials. However, the development team recently installed Python interpreters and various open-source libraries directly from PyPI and GitHub repositories on production servers without formal approval documentation to accelerate a critical customer deliverable, bypassing the standard software request process that requires written approval for non-standard software installations.\\n\\nPolicies referenced: policy_CM-11, policy_SI-12.3, policy_SC-12.2, policy_PE-5.2\",\n",
      "    \"is-compliant\": false,\n",
      "    \"non-compliant-reason\": \"This scenario violates CM-11 RULE-07 which requires users to obtain written approval before installing non-standard software. The development team installed Python interpreters and open-source libraries from PyPI and GitHub without formal approval documentation, bypassing the standard software request process. While the organization has comprehensive policies and controls in place for software installation, the specific installation of development tools without written approval constitutes a policy violation.\"\n",
      "  },\n",
      "  {\n",
      "    \"scenario-id\": \"scenario-id-8\",\n",
      "    \"scenario-detail\": \"CyberGuard Financial Services, a regional banking institution with 3,200 employees serving 500,000+ customers across 180 branch locations, maintains strict compliance with federal banking regulations and cybersecurity frameworks. The organization implemented comprehensive software installation governance per CM-11 with written policies defining permitted software sources including approved vendor portals, security patch repositories, and organizational app stores. Automated enforcement mechanisms through Microsoft Intune and endpoint detection systems prevent unauthorized installations, with monthly compliance reviews generating violation reports. All software installation attempts are logged with detailed audit trails including user identity, timestamp, software details, and approval status. The company's secure information disposal program follows SI-12.3 requirements using NIST 800-88 compliant methods for customer financial records, transaction data, and employee information with documented retention schedules and automated disposal triggers. High-sensitivity PII disposal requires certified destruction services with certificates of destruction, and all disposal activities are logged with responsible party identification. Cryptographic operations implement SC-12.2 controls using FIPS 140-2 validated key management systems from IBM and Entrust for symmetric key production, control, and distribution supporting customer transaction encryption, secure communications, and data protection. The key management infrastructure maintains current FIPS validation status with quarterly assessments and comprehensive documentation. Physical output device security per PE-5.2 requires smart card authentication on all 340 network printers, copiers, and multifunction devices across branch locations and corporate facilities, with failed authentication alerts and automatic document purging after 4 hours for sensitive materials. However, the organization's legacy mainframe system containing 15 years of archived customer transaction records continues operating beyond the 7-year regulatory retention period without formal disposal, as the disposal activities have been delayed for 8 months past the retention expiration date due to ongoing system migration planning, with no legal holds currently active on the archived data.\\n\\nPolicies referenced: policy_CM-11, policy_SI-12.3, policy_SC-12.2, policy_PE-5.2\",\n",
      "    \"is-compliant\": false,\n",
      "    \"non-compliant-reason\": \"This scenario violates SI-12.3 RULE-02 which requires information disposal to occur within 30 days after the established retention period expires unless legal hold applies. The archived customer transaction records have exceeded their 7-year retention period by 8 months (240 days) without disposal, and no legal holds are active. The 30-day disposal deadline has been significantly exceeded, making this a clear policy violation despite the organization's otherwise comprehensive compliance program.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8371d-106d-4893-bcdb-965bef86e4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
