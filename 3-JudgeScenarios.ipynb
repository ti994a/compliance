{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae2603e-4fea-446e-9216-700fc15171c2",
   "metadata": {},
   "source": [
    "## ScenarioJudger\n",
    "\n",
    " - Reads a file from S3 containing json compliance scenarios of the format:\n",
    "```json\n",
    "{\n",
    "  \"scenarios\": [\n",
    "    {\n",
    "      \"scenario-id\": \"scenario-id-1\",\n",
    "      \"scenario-detail\": \"A new employee, Sarah Johnson, joins the IT department...\",\n",
    "      \"is-compliant\": false,\n",
    "      \"non-compliant-reason\": \"The scenario violates...\" \n",
    "    },\n",
    "    {\n",
    "      \"scenario-id\": \"scenario-id-2\", \n",
    "      \"scenario-detail\": \"TechCorp implements a comprehensive incident response procedure...\",\n",
    "      \"is-compliant\": true,\n",
    "      \"non-compliant-reason\": \"\" \n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    " - Evaluates the veracity each scenario-detail based on RAGed NIST-based policies in Bedrock knowledgebase, comparing its determination against \"is-compliant\" in the json.\n",
    " - When its determination differs, generates json records:\n",
    "```json\n",
    "{\n",
    "  \"scenarios\": [\n",
    "    {\n",
    "      \"scenario-id\": \"scenario-id-1\",\n",
    "      \"scenario-detail\": \"A new employee, Sarah Johnson, joins the IT department...\",\n",
    "      \"is-compliant\": false,\n",
    "      \"non-compliant-reason\": \"The scenario violates...\",\n",
    "      \"judged-compliant\": true,\n",
    "      \"judged-compliant-reason\": \"Considered the rules AC...  and scenario is not in violation...\"\n",
    "      \"llm-judge\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "      \"judged-dtm\":  \n",
    "    },\n",
    "    {\n",
    "      \"scenario-id\": \"scenario-id-2\", \n",
    "      \"scenario-detail\": \"TechCorp implements a comprehensive incident response procedure...\",\n",
    "      \"is-compliant\": true,\n",
    "      \"non-compliant-reason\": \"\", \n",
    "      \"judged-compliant\": false,\n",
    "      \"judged-compliant-reason\": \"Scenario violates access control policy...\",\n",
    "      \"llm-judge\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "      \"judged-dtm\":   \n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    " - Stores json records back to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "execution_state": "idle",
   "id": "721c6452-39b4-4d10-875f-87c1fb55f2df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:32.700336Z",
     "iopub.status.busy": "2026-01-26T15:26:32.700073Z",
     "iopub.status.idle": "2026-01-26T15:26:32.712225Z",
     "shell.execute_reply": "2026-01-26T15:26:32.710688Z",
     "shell.execute_reply.started": "2026-01-26T15:26:32.700316Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import time   # For rate limiting between API calls\n",
    "import threading\n",
    "from botocore.exceptions import ClientError\n",
    "from compliance_utils import compliance_calculator, CALCULATOR_TOOL, COMPLIANCE_JUDGE_PROMPT\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Callable, Any\n",
    "\n",
    "FOLDER_HOME: Path = Path('/home/sagemaker-user')\n",
    "FOLDER_JUDGED_SCENARIOS: Path = FOLDER_HOME / 'data/judged_scenarios/'\n",
    "BUCKET = '183023889407-us-east-1-compliance-rule-generator'\n",
    "S3_SOURCE_SCENARIOS = 'scenarios/'  # Folder path in S3 where scenarios are stored\n",
    "S3_SOURCE_POLICY_ALL = 'policies/markdown/all-policies-main/'\n",
    "S3_JUDGED_SCENARIOS = 'scenarios-judged/'  # Folder path for results\n",
    "AWS_REGION = 'us-east-1'\n",
    "# KNOWLEDGE_BASE_ID = 'T8EW10IU3Z' - using s3 to retrieve policies, KB performance unsatisfactory\n",
    "MAX_TOKENS = 4096\n",
    "MAX_RETRIES_ON_THROTTLE = 5\n",
    "\n",
    "# Tool configuration for Bedrock Converse API\n",
    "# Forces the model to return structured JSON with specific schema, and use calculator tool\n",
    "TOOL_CONFIG = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"judged_scenario_json\",\n",
    "                \"description\": \"Return judged compliance scenarios as JSON\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"scenarios\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"judged-compliant\": {\"type\": \"boolean\"},\n",
    "                                        \"judged-compliant-reason\": {\"type\": \"string\"}\n",
    "                                    },\n",
    "                                    \"required\": [\"judged-compliant\", \"judged-compliant-reason\"]\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"scenarios\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"compliance_calculator\",\n",
    "                \"description\": \"Calculate and compare values with time, money, data, and percentage units\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"expression\": {\"type\": \"string\", \"description\": \"Expression like '800ms < 1s' or '4m > 3b'\"}\n",
    "                        },\n",
    "                        \"required\": [\"expression\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "MODELS = [\n",
    "        {\n",
    "            'name': 'claude_3_7_sonnet',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
    "            'temperature': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'claude_4_sonnet',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0',\n",
    "            'temperature': 0.0\n",
    "        },\n",
    "        {\n",
    "            'name': 'claude_opus_4_5',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/global.anthropic.claude-opus-4-5-20251101-v1:0',\n",
    "            'temperature': 0.0\n",
    "        },\n",
    "        {\n",
    "            'name': 'nova_premier',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-premier-v1:0',\n",
    "            'temperature': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'nova_2_lite',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-2-lite-v1:0',\n",
    "            'temperature': 0.2\n",
    "        }\n",
    "    ]\n",
    "\n",
    "SCENARIOS = [\"scenarios-8-policies-each.json\", \"scenarios-6-policies-each.json\"]\n",
    "\n",
    "# CALCULATOR_TOOL[\"toolSpec\"] references the calculator tool definition from compliance_calculator.py\n",
    "\n",
    "# Initialize AWS Bedrock clients\n",
    "# bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name=AWS_REGION)  # For knowledge base retrieval\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=AWS_REGION)  # For model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "execution_state": "idle",
   "id": "a72acffd-ba5a-4a18-99da-f41b6f65cec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:33.049313Z",
     "iopub.status.busy": "2026-01-26T15:26:33.049040Z",
     "iopub.status.idle": "2026-01-26T15:26:33.053574Z",
     "shell.execute_reply": "2026-01-26T15:26:33.052205Z",
     "shell.execute_reply.started": "2026-01-26T15:26:33.049291Z"
    }
   },
   "outputs": [],
   "source": [
    "def keep_alive():\n",
    "    while True:\n",
    "        time.sleep(300)  # 5 minutes\n",
    "        print(f\"Keep alive thread still running... {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "execution_state": "idle",
   "id": "4f6cb3ad-046d-4bee-982c-00399c7bbc74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:33.428935Z",
     "iopub.status.busy": "2026-01-26T15:26:33.428651Z",
     "iopub.status.idle": "2026-01-26T15:26:33.434429Z",
     "shell.execute_reply": "2026-01-26T15:26:33.433327Z",
     "shell.execute_reply.started": "2026-01-26T15:26:33.428914Z"
    }
   },
   "outputs": [],
   "source": [
    "def bedrock_call_with_retry(func: Callable[[], Any], max_retries: int = MAX_RETRIES_ON_THROTTLE, base_delay: int = 2) -> Any:\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'ThrottlingException':\n",
    "                delay = base_delay * (2 ** attempt) # exponential backoff in the event of throttling\n",
    "                print(f\"Rate limit hit, waiting {delay}s...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "    raise Exception(\"Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "execution_state": "idle",
   "id": "1abdea2c-e311-4358-8b11-3f02472ad8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:33.849920Z",
     "iopub.status.busy": "2026-01-26T15:26:33.849649Z",
     "iopub.status.idle": "2026-01-26T15:26:33.854604Z",
     "shell.execute_reply": "2026-01-26T15:26:33.853347Z",
     "shell.execute_reply.started": "2026-01-26T15:26:33.849898Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_scenarios_from_s3(input_bucket: str = BUCKET, s3_source_scenarios: str = S3_SOURCE_SCENARIOS, object_name: str = \"scenarios.json\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load scenarios from S3 JSON file.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=input_bucket, Key=s3_source_scenarios+object_name)\n",
    "    json_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    return json_data[\"scenarios\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "execution_state": "idle",
   "id": "fa5e2795-d84a-452f-9dba-a9ede04e259f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:34.261802Z",
     "iopub.status.busy": "2026-01-26T15:26:34.261525Z",
     "iopub.status.idle": "2026-01-26T15:26:34.266260Z",
     "shell.execute_reply": "2026-01-26T15:26:34.265115Z",
     "shell.execute_reply.started": "2026-01-26T15:26:34.261781Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_file_to_s3(file_path: Path, output_bucket: str = BUCKET, s3_key: str = None):\n",
    "    \"\"\"\n",
    "    Upload a local file to S3.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file(str(file_path), output_bucket, s3_key)\n",
    "    print(f\"Uploaded {file_path} to s3://{output_bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "execution_state": "idle",
   "id": "2e937950-914d-4f2e-88bb-0d757125f96a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:35.177716Z",
     "iopub.status.busy": "2026-01-26T15:26:35.177444Z",
     "iopub.status.idle": "2026-01-26T15:26:35.183126Z",
     "shell.execute_reply": "2026-01-26T15:26:35.181666Z",
     "shell.execute_reply.started": "2026-01-26T15:26:35.177695Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_policies_by_id(bucket:str, folder:str, policy_ids: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve specific policy documents from s3.\n",
    "    \"\"\"\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    policies = []\n",
    "    for policy_id in policy_ids:\n",
    "        response = s3.get_object(Bucket=bucket, Key=folder + policy_id + \".md\")\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        policies.append(f\"{policy_id}:\\n{content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(policies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "execution_state": "idle",
   "id": "64274281-02c9-467a-8ab9-e959c950f919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:35.837449Z",
     "iopub.status.busy": "2026-01-26T15:26:35.837165Z",
     "iopub.status.idle": "2026-01-26T15:26:35.843124Z",
     "shell.execute_reply": "2026-01-26T15:26:35.841792Z",
     "shell.execute_reply.started": "2026-01-26T15:26:35.837427Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_policies_for_scenario(scenario: Dict) -> str:\n",
    "    \"\"\"Extract policy IDs from scenario and retrieve policy content from S3\"\"\"\n",
    "    \n",
    "    # Extract policy IDs from scenario text\n",
    "    policy_match = re.search(r'Policies referenced: (.+)', scenario[\"scenario-detail\"])\n",
    "    if not policy_match:\n",
    "        print(f\"No policies referenced in scenario: {scenario['scenario-detail']}\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Parse policy IDs\n",
    "    policy_ids = [p.strip() for p in policy_match.group(1).split(',')]\n",
    "    \n",
    "    # Retrieve policy documents from S3\n",
    "    s3 = boto3.client('s3')\n",
    "    policies = []\n",
    "    for policy_id in policy_ids:\n",
    "        response = s3.get_object(Bucket=BUCKET, Key=S3_SOURCE_POLICY_ALL + policy_id + \".md\")\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        policies.append(f\"{policy_id}:\\n{content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(policies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "execution_state": "idle",
   "id": "464020d0-7465-4541-84c3-55620b1f78c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:36.917162Z",
     "iopub.status.busy": "2026-01-26T15:26:36.916801Z",
     "iopub.status.idle": "2026-01-26T15:26:36.928105Z",
     "shell.execute_reply": "2026-01-26T15:26:36.926641Z",
     "shell.execute_reply.started": "2026-01-26T15:26:36.917133Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_with_claude(\n",
    "    prompt: str,\n",
    "    model_id: str,\n",
    "    temperature: float\n",
    ") -> Dict:\n",
    "    \n",
    "    messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    result = {}\n",
    "    \n",
    "    while True:\n",
    "        response = bedrock_call_with_retry(\n",
    "            lambda: bedrock_runtime.converse(modelId=model_id, messages=messages,toolConfig=TOOL_CONFIG,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": MAX_TOKENS,\n",
    "                    \"temperature\": temperature \n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # track per-scenario token usage\n",
    "        usage = response.get('usage', {})\n",
    "        input_tokens += usage.get('inputTokens', 0)\n",
    "        output_tokens += usage.get('outputTokens', 0)\n",
    "        \n",
    "        if response['stopReason'] == 'tool_use':\n",
    "            tool_results = []\n",
    "            for content_block in response['output']['message']['content']:\n",
    "                if 'toolUse' in content_block:\n",
    "                    tool_name = content_block['toolUse']['name']\n",
    "                    tool_use_id = content_block['toolUse']['toolUseId']\n",
    "                    \n",
    "                    if tool_name == 'compliance_calculator':\n",
    "                        expression = content_block['toolUse']['input']['expression']                         \n",
    "                        calc_result = compliance_calculator(expression)\n",
    "                        # print(\"=\" * 60)\n",
    "                        # print(f\"Compliance calculator expression: {expression}\")\n",
    "                        # print(f\"Compliance calculator result: {calc_result}\")\n",
    "                        # print(\"=\" * 60)\n",
    "                        tool_results.append({\n",
    "                            \"toolResult\": {\n",
    "                                \"toolUseId\": tool_use_id,\n",
    "                                \"content\": [{\"text\": calc_result}]\n",
    "                            }\n",
    "                        })\n",
    "                    elif tool_name == 'judged_scenario_json':\n",
    "                        tool_result = content_block['toolUse']['input']\n",
    "                         # Parse JSON if it's a string\n",
    "                        if isinstance(tool_result, str):\n",
    "                            tool_result = json.loads(tool_result)\n",
    "                        result[\"judged-compliant\"] = tool_result['scenarios'][0]['judged-compliant']\n",
    "                        result[\"judged-compliant-reason\"] = tool_result['scenarios'][0]['judged-compliant-reason']\n",
    "                        break\n",
    "            \n",
    "            if tool_results:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": response['output']['message']['content']})\n",
    "                messages.append({\"role\": \"user\", \"content\": tool_results})\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    result.update({\n",
    "        \"llm-judge-input-tokens\" : input_tokens,\n",
    "        \"llm-judge-output-tokens\": output_tokens,\n",
    "        \"llm-judge-total-tokens\" : input_tokens + output_tokens\n",
    "    })\n",
    "        \n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "execution_state": "idle",
   "id": "e5889b17-5c28-4da7-a7a5-10c5c07a88d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:37.729028Z",
     "iopub.status.busy": "2026-01-26T15:26:37.728674Z",
     "iopub.status.idle": "2026-01-26T15:26:37.737419Z",
     "shell.execute_reply": "2026-01-26T15:26:37.736286Z",
     "shell.execute_reply.started": "2026-01-26T15:26:37.729000Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_with_nova(prompt: str, model_id: str, temperature: float) -> Dict:\n",
    "    enhanced_prompt = f\"\"\"{prompt}\n",
    "\n",
    "Respond with a JSON object in this exact format:\n",
    "{{\n",
    "  \"judged-compliant\": true/false,\n",
    "  \"judged-compliant-reason\": \"detailed explanation\"\n",
    "}}\"\"\"\n",
    "    \n",
    "    body = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": enhanced_prompt}]}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": MAX_TOKENS,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = bedrock_call_with_retry(\n",
    "        lambda: bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    content = response_body['output']['message']['content'][0]['text']\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    try:\n",
    "        # Find JSON in response text\n",
    "        start = content.find('{')\n",
    "        end = content.rfind('}') + 1\n",
    "        json_str = content[start:end]\n",
    "        result = json.loads(json_str)\n",
    "        \n",
    "        return {\n",
    "            \"judged-compliant\": result[\"judged-compliant\"],\n",
    "            \"judged-compliant-reason\": result[\"judged-compliant-reason\"],\n",
    "            \"llm-judge-input-tokens\": response_body.get('usage', {}).get('inputTokens', 0),\n",
    "            \"llm-judge-output-tokens\": response_body.get('usage', {}).get('outputTokens', 0),\n",
    "            \"llm-judge-total-tokens\": response_body.get('usage', {}).get('inputTokens', 0) + response_body.get('usage', {}).get('outputTokens', 0)\n",
    "        }\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        return {\n",
    "            \"judged-compliant\": False,\n",
    "            \"judged-compliant-reason\": f\"Error parsing Nova response: {str(e)}\",\n",
    "            \"llm-judge-input-tokens\": 0,\n",
    "            \"llm-judge-output-tokens\": 0,\n",
    "            \"llm-judge-total-tokens\": 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "execution_state": "idle",
   "id": "01b553aa-0099-445e-a80e-afed0fcbef57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:38.803607Z",
     "iopub.status.busy": "2026-01-26T15:26:38.803118Z",
     "iopub.status.idle": "2026-01-26T15:26:38.811006Z",
     "shell.execute_reply": "2026-01-26T15:26:38.809745Z",
     "shell.execute_reply.started": "2026-01-26T15:26:38.803572Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_scenarios(\n",
    "    source_scenarios: List[Dict],\n",
    "    model_arn: str, \n",
    "    temperature: float = None,\n",
    "    max_scenarios: int = None\n",
    ") -> List[Dict]:\n",
    "    \n",
    "    # Extract model ID and detect model type\n",
    "    model_id = model_arn.split('/')[-1] if '/' in model_arn else model_arn\n",
    "    is_nova_model = \"nova\" in model_id.lower()\n",
    "    \n",
    "    judged_scenarios = []\n",
    "    for scenario in source_scenarios[:max_scenarios] if max_scenarios else source_scenarios:\n",
    "        \n",
    "        # Get policy context from S3 for this scenario\n",
    "        retrieved_policies = get_policies_for_scenario(scenario)\n",
    "        if not retrieved_policies:\n",
    "            continue  # Skip scenarios without policy references\n",
    "        \n",
    "        prompt = COMPLIANCE_JUDGE_PROMPT.format(\n",
    "            retrieved_policies=retrieved_policies,\n",
    "            scenario_detail=scenario[\"scenario-detail\"]\n",
    "        )\n",
    "        \n",
    "        if is_nova_model:\n",
    "            # Nova: Use invoke_model with JSON response\n",
    "            judged_scenario_detail = judge_with_nova(prompt, model_id, temperature)\n",
    "            print(\"Calling judge_with_nova\")\n",
    "        else:\n",
    "            # Claude: Use converse with tools\n",
    "            judged_scenario_detail = judge_with_claude(prompt, model_id, temperature)\n",
    "            print(\"Calling judge_with_claude\")\n",
    "        \n",
    "        # Add metadata\n",
    "        judged_scenario = scenario.copy()\n",
    "        judged_scenario.update(judged_scenario_detail)\n",
    "        judged_scenario.update({\n",
    "            \"judged-dtm\": datetime.datetime.now().isoformat(),\n",
    "            \"llm-judge\": model_id,\n",
    "            \"llm-judge-temp\": temperature\n",
    "        })\n",
    "        \n",
    "        judged_scenarios.append(judged_scenario)\n",
    "    \n",
    "    return judged_scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "execution_state": "idle",
   "id": "aaf8bcc2-d0f7-40bb-8c04-17f9981de4fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:40.337010Z",
     "iopub.status.busy": "2026-01-26T15:26:40.336742Z",
     "iopub.status.idle": "2026-01-26T15:26:40.354201Z",
     "shell.execute_reply": "2026-01-26T15:26:40.352697Z",
     "shell.execute_reply.started": "2026-01-26T15:26:40.336990Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_scenarios_streaming(\n",
    "    source_scenarios: List[Dict],\n",
    "    model_arn: str, \n",
    "    temperature: float = None,\n",
    "    max_scenarios: int = None,\n",
    "    output_file: Path = None\n",
    ") -> List[Dict]:\n",
    "    \n",
    "    model_id = model_arn.split('/')[-1] if '/' in model_arn else model_arn\n",
    "    is_nova_model = \"nova\" in model_id.lower()\n",
    "    \n",
    "    # Initialize file with opening bracket\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('{\"scenarios\":[')\n",
    "    \n",
    "    judged_scenarios = []\n",
    "    scenarios_to_process = source_scenarios[:max_scenarios] if max_scenarios else source_scenarios\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios_to_process):\n",
    "        retrieved_policies = get_policies_for_scenario(scenario)\n",
    "        if not retrieved_policies:\n",
    "            continue\n",
    "        \n",
    "        prompt = COMPLIANCE_JUDGE_PROMPT.format(\n",
    "            retrieved_policies=retrieved_policies,\n",
    "            scenario_detail=scenario[\"scenario-detail\"]\n",
    "        )\n",
    "        \n",
    "        if is_nova_model:\n",
    "            judged_scenario_detail = judge_with_nova(prompt, model_id, temperature)\n",
    "        else:\n",
    "            judged_scenario_detail = judge_with_claude(prompt, model_id, temperature)\n",
    "        \n",
    "        judged_scenario = scenario.copy()\n",
    "        judged_scenario.update(judged_scenario_detail)\n",
    "        judged_scenario.update({\n",
    "            \"judged-dtm\": datetime.datetime.now().isoformat(),\n",
    "            \"llm-judge\": model_id,\n",
    "            \"llm-judge-temp\": temperature\n",
    "        })\n",
    "        \n",
    "        judged_scenarios.append(judged_scenario)\n",
    "        \n",
    "        # Append to file immediately\n",
    "        with open(output_file, 'a') as f:\n",
    "            if i > 0:  # Add comma before all but first scenario\n",
    "                f.write(',')\n",
    "            json.dump(judged_scenario, f)\n",
    "        \n",
    "        print(f\"Saved scenario {i+1}/{len(scenarios_to_process)}\")\n",
    "    \n",
    "    # Close the JSON structure\n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(']}')\n",
    "    \n",
    "    return judged_scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "execution_state": "idle",
   "id": "b17b51d7-4d3c-4258-bd1b-58e80adb8763",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:26:41.766594Z",
     "iopub.status.busy": "2026-01-26T15:26:41.766141Z",
     "iopub.status.idle": "2026-01-26T15:26:41.772791Z",
     "shell.execute_reply": "2026-01-26T15:26:41.771543Z",
     "shell.execute_reply.started": "2026-01-26T15:26:41.766561Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_scenarios_to_file(scenarios: List[Dict], output_path: Path):\n",
    "    \n",
    "    # Print scenarios to console for immediate review\n",
    "    print(json.dumps(scenarios, indent=2))\n",
    "\n",
    "    # Create parent directories if they don't exist\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save to file with metadata and statistics\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'total_scenarios': len(scenarios),\n",
    "            'compliant_count': sum(1 for s in scenarios if s['is-compliant']),\n",
    "            'non_compliant_count': sum(1 for s in scenarios if not s['is-compliant']),\n",
    "            'judged_compliant_count': sum(1 for s in scenarios if s['judged-compliant']),\n",
    "            'judged_non_compliant_count': sum(1 for s in scenarios if not s['judged-compliant']),\n",
    "            'scenarios': scenarios\n",
    "        }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "execution_state": "idle",
   "id": "8b5a5088-bba4-4786-82b1-44374c091b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:37:21.599007Z",
     "iopub.status.busy": "2026-01-26T15:37:21.598631Z",
     "iopub.status.idle": "2026-01-26T15:37:21.610202Z",
     "shell.execute_reply": "2026-01-26T15:37:21.608770Z",
     "shell.execute_reply.started": "2026-01-26T15:37:21.598970Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(models=None, scenarios=None):\n",
    "\n",
    "    # Start keep-alive thread\n",
    "    # daemon=True parameter makes the thread automatically stop when the main program ends\n",
    "    thread = threading.Thread(target=keep_alive, daemon=True)\n",
    "    thread.start()\n",
    "    \n",
    "    if not models or not scenarios:\n",
    "        print(\"Error: Both models and scenarios must be provided\")\n",
    "        return\n",
    "\n",
    "    # Get model names for validation\n",
    "    model_names = [m['name'] for m in MODELS]\n",
    "    \n",
    "    # Handle \"all\" and convert to lists\n",
    "    models = model_names if str(models).lower() == \"all\" else [models] if isinstance(models, str) else models\n",
    "    scenarios = SCENARIOS if str(scenarios).lower() == \"all\" else [scenarios] if isinstance(scenarios, str) else scenarios\n",
    "    \n",
    "    # Validate models\n",
    "    if invalid := [m for m in models if m not in model_names]:\n",
    "        print(f\"Invalid models: {invalid}. Valid: {model_names}\")\n",
    "        return\n",
    "\n",
    "    judged_scenarios_batch = \"\"\n",
    "    \n",
    "     # Run all specified combinations\n",
    "    for model_name in models:\n",
    "        for scenario_file in scenarios:\n",
    "            try:    \n",
    "                model = next(m for m in MODELS if m['name'] == model_name)\n",
    "                source_scenarios = load_scenarios_from_s3(BUCKET, S3_SOURCE_SCENARIOS, scenario_file)\n",
    "                print(f\"Processing: {model_name} with {scenario_file}\")\n",
    "            \n",
    "                judged_scenarios_batch = f\"judged_scenarios_batch-{scenario_file.split('.')[0]}-{model['name']}-temp{model['temperature']}\"\n",
    "                print (\"=\" * 100)\n",
    "                print(f\"Starting batch: {judged_scenarios_batch}\")\n",
    "                print (\"=\" * 100)\n",
    "\n",
    "                judged_scenarios_file = f\"{judged_scenarios_batch}.json\"\n",
    "                local_file_path: Path = FOLDER_JUDGED_SCENARIOS / judged_scenarios_file\n",
    "                s3_key = S3_JUDGED_SCENARIOS + judged_scenarios_file\n",
    "                \n",
    "                judged_scenarios = judge_scenarios_streaming(\n",
    "                    source_scenarios = source_scenarios,\n",
    "                    model_arn = model['arn'],\n",
    "                    temperature = model['temperature'],\n",
    "                    max_scenarios = 200,\n",
    "                    output_file = local_file_path\n",
    "                )\n",
    "\n",
    "                save_file_to_s3(local_file_path, BUCKET, s3_key)\n",
    "                \n",
    "                print (\"=\" * 100)\n",
    "                print(f\"Finished batch: {judged_scenarios_batch}: {len(judged_scenarios)} scenarios processed\")\n",
    "                print (\"=\" * 100)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {judged_scenarios_batch}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()  # show  full error trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "execution_state": "idle",
   "id": "c0fae162-5344-421d-9ab6-29dfef33dcdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:37:22.348265Z",
     "iopub.status.busy": "2026-01-26T15:37:22.347998Z",
     "iopub.status.idle": "2026-01-26T16:40:12.319826Z",
     "shell.execute_reply": "2026-01-26T16:40:12.318398Z",
     "shell.execute_reply.started": "2026-01-26T15:37:22.348244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: nova_premier with scenarios-8-policies-each.json\n",
      "====================================================================================================\n",
      "Starting batch: judged_scenarios_batch-scenarios-8-policies-each-nova_premier-temp0.1\n",
      "====================================================================================================\n",
      "Saved scenario 1/200\n",
      "Saved scenario 2/200\n",
      "Saved scenario 3/200\n",
      "Saved scenario 4/200\n",
      "Keep alive thread still running... 2026-01-26 15:38:37\n",
      "Saved scenario 5/200\n",
      "Keep alive thread still running... 2026-01-26 15:38:47\n",
      "Saved scenario 6/200\n",
      "Keep alive thread still running... 2026-01-26 15:39:03\n",
      "Saved scenario 7/200\n",
      "Keep alive thread still running... 2026-01-26 15:39:20\n",
      "Saved scenario 8/200\n",
      "Saved scenario 9/200\n",
      "Saved scenario 10/200\n",
      "Saved scenario 11/200\n",
      "Saved scenario 12/200\n",
      "Keep alive thread still running... 2026-01-26 15:40:26\n",
      "Saved scenario 13/200\n",
      "Saved scenario 14/200\n",
      "Saved scenario 15/200\n",
      "Saved scenario 16/200\n",
      "Keep alive thread still running... 2026-01-26 15:41:46\n",
      "Saved scenario 17/200\n",
      "Keep alive thread still running... 2026-01-26 15:42:00\n",
      "Saved scenario 18/200\n",
      "Keep alive thread still running... 2026-01-26 15:42:22\n",
      "Saved scenario 19/200\n",
      "Saved scenario 20/200\n",
      "Keep alive thread still running... 2026-01-26 15:43:37\n",
      "Keep alive thread still running... 2026-01-26 15:43:47\n",
      "Saved scenario 21/200\n",
      "Keep alive thread still running... 2026-01-26 15:44:03\n",
      "Saved scenario 22/200\n",
      "Keep alive thread still running... 2026-01-26 15:44:20\n",
      "Saved scenario 23/200\n",
      "Saved scenario 24/200\n",
      "Saved scenario 25/200\n",
      "Saved scenario 26/200\n",
      "Keep alive thread still running... 2026-01-26 15:45:26\n",
      "Saved scenario 27/200\n",
      "Saved scenario 28/200\n",
      "Saved scenario 29/200\n",
      "Saved scenario 30/200\n",
      "Keep alive thread still running... 2026-01-26 15:46:46\n",
      "Saved scenario 31/200\n",
      "Keep alive thread still running... 2026-01-26 15:47:00\n",
      "Saved scenario 32/200\n",
      "Keep alive thread still running... 2026-01-26 15:47:22\n",
      "Saved scenario 33/200\n",
      "Saved scenario 34/200\n",
      "Saved scenario 35/200\n",
      "Saved scenario 36/200\n",
      "Keep alive thread still running... 2026-01-26 15:48:37\n",
      "Saved scenario 37/200\n",
      "Keep alive thread still running... 2026-01-26 15:48:47\n",
      "Saved scenario 38/200\n",
      "Keep alive thread still running... 2026-01-26 15:49:03\n",
      "Saved scenario 39/200\n",
      "Saved scenario 40/200\n",
      "Keep alive thread still running... 2026-01-26 15:49:20\n",
      "Saved scenario 41/200\n",
      "Saved scenario 42/200\n",
      "Saved scenario 43/200\n",
      "Saved scenario 44/200\n",
      "Keep alive thread still running... 2026-01-26 15:50:26\n",
      "Saved scenario 45/200\n",
      "Saved scenario 46/200\n",
      "Saved scenario 47/200\n",
      "Saved scenario 48/200\n",
      "Saved scenario 49/200\n",
      "Saved scenario 50/200\n",
      "Keep alive thread still running... 2026-01-26 15:51:46\n",
      "Saved scenario 51/200\n",
      "Saved scenario 52/200\n",
      "Keep alive thread still running... 2026-01-26 15:52:00\n",
      "Saved scenario 53/200\n",
      "Keep alive thread still running... 2026-01-26 15:52:22\n",
      "Saved scenario 54/200\n",
      "Saved scenario 55/200\n",
      "Saved scenario 56/200\n",
      "Saved scenario 57/200\n",
      "Keep alive thread still running... 2026-01-26 15:53:37\n",
      "Saved scenario 58/200\n",
      "Keep alive thread still running... 2026-01-26 15:53:47\n",
      "Keep alive thread still running... 2026-01-26 15:54:03\n",
      "Saved scenario 59/200\n",
      "Keep alive thread still running... 2026-01-26 15:54:20\n",
      "Saved scenario 60/200\n",
      "Saved scenario 61/200\n",
      "Saved scenario 62/200\n",
      "Keep alive thread still running... 2026-01-26 15:55:26\n",
      "Saved scenario 63/200\n",
      "Saved scenario 64/200\n",
      "Saved scenario 65/200\n",
      "Saved scenario 66/200\n",
      "Keep alive thread still running... 2026-01-26 15:56:46\n",
      "Saved scenario 67/200\n",
      "Keep alive thread still running... 2026-01-26 15:57:00\n",
      "Saved scenario 68/200\n",
      "Keep alive thread still running... 2026-01-26 15:57:22\n",
      "Saved scenario 69/200\n",
      "Saved scenario 70/200\n",
      "Saved scenario 71/200\n",
      "Saved scenario 72/200\n",
      "Keep alive thread still running... 2026-01-26 15:58:37\n",
      "Saved scenario 73/200\n",
      "Keep alive thread still running... 2026-01-26 15:58:47\n",
      "Keep alive thread still running... 2026-01-26 15:59:03\n",
      "Saved scenario 74/200\n",
      "Keep alive thread still running... 2026-01-26 15:59:20\n",
      "Saved scenario 75/200\n",
      "Saved scenario 76/200\n",
      "Saved scenario 77/200\n",
      "Keep alive thread still running... 2026-01-26 16:00:26\n",
      "Saved scenario 78/200\n",
      "Saved scenario 79/200\n",
      "Saved scenario 80/200\n",
      "Saved scenario 81/200\n",
      "Keep alive thread still running... 2026-01-26 16:01:46\n",
      "Keep alive thread still running... 2026-01-26 16:02:00\n",
      "Saved scenario 82/200\n",
      "Keep alive thread still running... 2026-01-26 16:02:22\n",
      "Saved scenario 83/200\n",
      "Saved scenario 84/200\n",
      "Keep alive thread still running... 2026-01-26 16:03:37\n",
      "Saved scenario 85/200\n",
      "Keep alive thread still running... 2026-01-26 16:03:47\n",
      "Saved scenario 86/200\n",
      "Keep alive thread still running... 2026-01-26 16:04:03\n",
      "Keep alive thread still running... 2026-01-26 16:04:20\n",
      "Saved scenario 87/200\n",
      "Saved scenario 88/200\n",
      "Saved scenario 89/200\n",
      "Saved scenario 90/200\n",
      "Keep alive thread still running... 2026-01-26 16:05:26\n",
      "Saved scenario 91/200\n",
      "Saved scenario 92/200\n",
      "Saved scenario 93/200\n",
      "Saved scenario 94/200\n",
      "Saved scenario 95/200\n",
      "Keep alive thread still running... 2026-01-26 16:06:46\n",
      "Saved scenario 96/200\n",
      "Keep alive thread still running... 2026-01-26 16:07:00\n",
      "Saved scenario 97/200\n",
      "Keep alive thread still running... 2026-01-26 16:07:22\n",
      "Saved scenario 98/200\n",
      "Saved scenario 99/200\n",
      "Saved scenario 100/200\n",
      "Saved scenario 101/200\n",
      "Keep alive thread still running... 2026-01-26 16:08:37\n",
      "Saved scenario 102/200\n",
      "Keep alive thread still running... 2026-01-26 16:08:47\n",
      "Saved scenario 103/200\n",
      "Keep alive thread still running... 2026-01-26 16:09:03\n",
      "Saved scenario 104/200\n",
      "Keep alive thread still running... 2026-01-26 16:09:20\n",
      "Saved scenario 105/200\n",
      "Saved scenario 106/200\n",
      "Saved scenario 107/200\n",
      "Saved scenario 108/200\n",
      "Keep alive thread still running... 2026-01-26 16:10:26\n",
      "Saved scenario 109/200\n",
      "Saved scenario 110/200\n",
      "Saved scenario 111/200\n",
      "Saved scenario 112/200\n",
      "Saved scenario 113/200\n",
      "Keep alive thread still running... 2026-01-26 16:11:46\n",
      "Saved scenario 114/200\n",
      "Keep alive thread still running... 2026-01-26 16:12:00\n",
      "Saved scenario 115/200\n",
      "Keep alive thread still running... 2026-01-26 16:12:22\n",
      "Saved scenario 116/200\n",
      "Saved scenario 117/200\n",
      "Saved scenario 118/200\n",
      "Saved scenario 119/200\n",
      "Keep alive thread still running... 2026-01-26 16:13:37\n",
      "Saved scenario 120/200\n",
      "Keep alive thread still running... 2026-01-26 16:13:47\n",
      "Keep alive thread still running... 2026-01-26 16:14:03\n",
      "Saved scenario 121/200\n",
      "Keep alive thread still running... 2026-01-26 16:14:20\n",
      "Saved scenario 122/200\n",
      "Saved scenario 123/200\n",
      "Saved scenario 124/200\n",
      "Keep alive thread still running... 2026-01-26 16:15:26\n",
      "Saved scenario 125/200\n",
      "Saved scenario 126/200\n",
      "Saved scenario 127/200\n",
      "Saved scenario 128/200\n",
      "Saved scenario 129/200\n",
      "Keep alive thread still running... 2026-01-26 16:16:46\n",
      "Saved scenario 130/200\n",
      "Keep alive thread still running... 2026-01-26 16:17:00\n",
      "Saved scenario 131/200\n",
      "Keep alive thread still running... 2026-01-26 16:17:22\n",
      "Saved scenario 132/200\n",
      "Saved scenario 133/200\n",
      "Saved scenario 134/200\n",
      "Saved scenario 135/200\n",
      "Keep alive thread still running... 2026-01-26 16:18:37\n",
      "Saved scenario 136/200\n",
      "Keep alive thread still running... 2026-01-26 16:18:47\n",
      "Saved scenario 137/200\n",
      "Keep alive thread still running... 2026-01-26 16:19:03\n",
      "Saved scenario 138/200\n",
      "Keep alive thread still running... 2026-01-26 16:19:20\n",
      "Saved scenario 139/200\n",
      "Saved scenario 140/200\n",
      "Saved scenario 141/200\n",
      "Keep alive thread still running... 2026-01-26 16:20:26\n",
      "Saved scenario 142/200\n",
      "Saved scenario 143/200\n",
      "Saved scenario 144/200\n",
      "Saved scenario 145/200\n",
      "Keep alive thread still running... 2026-01-26 16:21:46\n",
      "Saved scenario 146/200\n",
      "Keep alive thread still running... 2026-01-26 16:22:00\n",
      "Keep alive thread still running... 2026-01-26 16:22:22\n",
      "Saved scenario 147/200\n",
      "Saved scenario 148/200\n",
      "Saved scenario 149/200\n",
      "Saved scenario 150/200\n",
      "Keep alive thread still running... 2026-01-26 16:23:37\n",
      "Keep alive thread still running... 2026-01-26 16:23:47\n",
      "Saved scenario 151/200\n",
      "Keep alive thread still running... 2026-01-26 16:24:03\n",
      "Keep alive thread still running... 2026-01-26 16:24:20\n",
      "Saved scenario 152/200\n",
      "Saved scenario 153/200\n",
      "Saved scenario 154/200\n",
      "Keep alive thread still running... 2026-01-26 16:25:26\n",
      "Saved scenario 155/200\n",
      "Saved scenario 156/200\n",
      "Saved scenario 157/200\n",
      "Keep alive thread still running... 2026-01-26 16:26:46\n",
      "Saved scenario 158/200\n",
      "Keep alive thread still running... 2026-01-26 16:27:00\n",
      "Saved scenario 159/200\n",
      "Keep alive thread still running... 2026-01-26 16:27:22\n",
      "Saved scenario 160/200\n",
      "Saved scenario 161/200\n",
      "Saved scenario 162/200\n",
      "Saved scenario 163/200\n",
      "Keep alive thread still running... 2026-01-26 16:28:37\n",
      "Keep alive thread still running... 2026-01-26 16:28:47\n",
      "Saved scenario 164/200\n",
      "Keep alive thread still running... 2026-01-26 16:29:03\n",
      "Saved scenario 165/200\n",
      "Saved scenario 166/200\n",
      "Keep alive thread still running... 2026-01-26 16:29:20\n",
      "Saved scenario 167/200\n",
      "Saved scenario 168/200\n",
      "Saved scenario 169/200\n",
      "Keep alive thread still running... 2026-01-26 16:30:26\n",
      "Saved scenario 170/200\n",
      "Saved scenario 171/200\n",
      "Saved scenario 172/200\n",
      "Saved scenario 173/200\n",
      "Keep alive thread still running... 2026-01-26 16:31:46\n",
      "Saved scenario 174/200\n",
      "Keep alive thread still running... 2026-01-26 16:32:00\n",
      "Saved scenario 175/200\n",
      "Saved scenario 176/200\n",
      "Keep alive thread still running... 2026-01-26 16:32:22\n",
      "Saved scenario 177/200\n",
      "Saved scenario 178/200\n",
      "Keep alive thread still running... 2026-01-26 16:33:37\n",
      "Saved scenario 179/200\n",
      "Keep alive thread still running... 2026-01-26 16:33:47\n",
      "Saved scenario 180/200\n",
      "Keep alive thread still running... 2026-01-26 16:34:03\n",
      "Saved scenario 181/200\n",
      "Keep alive thread still running... 2026-01-26 16:34:20\n",
      "Saved scenario 182/200\n",
      "Saved scenario 183/200\n",
      "Saved scenario 184/200\n",
      "Keep alive thread still running... 2026-01-26 16:35:26\n",
      "Saved scenario 185/200\n",
      "Saved scenario 186/200\n",
      "Saved scenario 187/200\n",
      "Saved scenario 188/200\n",
      "Keep alive thread still running... 2026-01-26 16:36:46\n",
      "Saved scenario 189/200\n",
      "Keep alive thread still running... 2026-01-26 16:37:00\n",
      "Saved scenario 190/200\n",
      "Keep alive thread still running... 2026-01-26 16:37:22\n",
      "Saved scenario 191/200\n",
      "Saved scenario 192/200\n",
      "Saved scenario 193/200\n",
      "Saved scenario 194/200\n",
      "Saved scenario 195/200\n",
      "Keep alive thread still running... 2026-01-26 16:38:37\n",
      "Keep alive thread still running... 2026-01-26 16:38:47\n",
      "Saved scenario 196/200\n",
      "Keep alive thread still running... 2026-01-26 16:39:03\n",
      "Keep alive thread still running... 2026-01-26 16:39:20\n",
      "Saved scenario 197/200\n",
      "Saved scenario 198/200\n",
      "Saved scenario 199/200\n",
      "Saved scenario 200/200\n",
      "Uploaded /home/sagemaker-user/data/judged_scenarios/judged_scenarios_batch-scenarios-8-policies-each-nova_premier-temp0.1.json to s3://183023889407-us-east-1-compliance-rule-generator/scenarios-judged/judged_scenarios_batch-scenarios-8-policies-each-nova_premier-temp0.1.json\n",
      "====================================================================================================\n",
      "Finished batch: judged_scenarios_batch-scenarios-8-policies-each-nova_premier-temp0.1: 200 scenarios processed\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "main(\"nova_premier\", \"scenarios-10-policies-each-200.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "execution_state": "idle",
   "id": "61d791d7-018d-4391-b2b6-b5242d108ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T21:55:53.423741Z",
     "iopub.status.busy": "2026-01-25T21:55:53.423463Z",
     "iopub.status.idle": "2026-01-25T21:55:53.687029Z",
     "shell.execute_reply": "2026-01-25T21:55:53.685839Z",
     "shell.execute_reply.started": "2026-01-25T21:55:53.423721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-2-11b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-2-3b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-2-90b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-2-1b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-1-8b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-1-70b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-3-70b-instruct-v1:0\n"
     ]
    }
   ],
   "source": [
    "bedrock_client = boto3.client('bedrock', region_name=AWS_REGION)\n",
    "response = bedrock_client.list_inference_profiles()\n",
    "for profile in response['inferenceProfileSummaries']:\n",
    "    if 'meta' in profile['inferenceProfileName'].lower():\n",
    "        print(f\"ARN: {profile['inferenceProfileArn']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736c07d-3bca-4aca-848e-3e66a07f9896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
