{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae2603e-4fea-446e-9216-700fc15171c2",
   "metadata": {},
   "source": [
    "## ScenarioJudger\n",
    "\n",
    " - Reads a file from S3 containing json compliance scenarios of the format:\n",
    "```json\n",
    "{\n",
    "  \"scenarios\": [\n",
    "    {\n",
    "      \"scenario-id\": \"scenario-id-1\",\n",
    "      \"scenario-detail\": \"A new employee, Sarah Johnson, joins the IT department...\",\n",
    "      \"is-compliant\": false,\n",
    "      \"non-compliant-reason\": \"The scenario violates...\" \n",
    "    },\n",
    "    {\n",
    "      \"scenario-id\": \"scenario-id-2\", \n",
    "      \"scenario-detail\": \"TechCorp implements a comprehensive incident response procedure...\",\n",
    "      \"is-compliant\": true,\n",
    "      \"non-compliant-reason\": \"\" \n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    " - Evaluates the veracity each scenario-detail based on RAGed NIST-based policies in Bedrock knowledgebase, comparing its determination against \"is-compliant\" in the json.\n",
    " - When its determination differs, generates json records:\n",
    "```json\n",
    "{\n",
    "  \"scenarios\": [\n",
    "    {\n",
    "      \"scenario-id\": \"scenario-id-1\",\n",
    "      \"scenario-detail\": \"A new employee, Sarah Johnson, joins the IT department...\",\n",
    "      \"is-compliant\": false,\n",
    "      \"non-compliant-reason\": \"The scenario violates...\",\n",
    "      \"judged-compliant\": true,\n",
    "      \"judged-compliant-reason\": \"Considered the rules AC...  and scenario is not in violation...\"\n",
    "      \"llm-judge\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "      \"judged-dtm\":  \n",
    "    },\n",
    "    {\n",
    "      \"scenario-id\": \"scenario-id-2\", \n",
    "      \"scenario-detail\": \"TechCorp implements a comprehensive incident response procedure...\",\n",
    "      \"is-compliant\": true,\n",
    "      \"non-compliant-reason\": \"\", \n",
    "      \"judged-compliant\": false,\n",
    "      \"judged-compliant-reason\": \"Scenario violates access control policy...\",\n",
    "      \"llm-judge\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "      \"judged-dtm\":   \n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    " - Stores json records back to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "721c6452-39b4-4d10-875f-87c1fb55f2df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:01:57.715754Z",
     "iopub.status.busy": "2026-01-26T00:01:57.715453Z",
     "iopub.status.idle": "2026-01-26T00:01:57.795901Z",
     "shell.execute_reply": "2026-01-26T00:01:57.794597Z",
     "shell.execute_reply.started": "2026-01-26T00:01:57.715730Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import time   # For rate limiting between API calls\n",
    "import threading\n",
    "from botocore.exceptions import ClientError\n",
    "from compliance_utils import compliance_calculator, CALCULATOR_TOOL, COMPLIANCE_JUDGE_PROMPT\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Callable, Any\n",
    "\n",
    "FOLDER_HOME: Path = Path('/home/sagemaker-user')\n",
    "FOLDER_JUDGED_SCENARIOS: Path = FOLDER_HOME / 'data/judged_scenarios/'\n",
    "BUCKET = '183023889407-us-east-1-compliance-rule-generator'\n",
    "S3_SOURCE_SCENARIOS = 'scenarios/'  # Folder path in S3 where scenarios are stored\n",
    "S3_SOURCE_POLICY_ALL = 'policies/markdown/all-policies-main/'\n",
    "S3_JUDGED_SCENARIOS = 'scenarios-judged/'  # Folder path for results\n",
    "AWS_REGION = 'us-east-1'\n",
    "# KNOWLEDGE_BASE_ID = 'T8EW10IU3Z' - using s3 to retrieve policies, KB performance unsatisfactory\n",
    "MAX_TOKENS = 4096\n",
    "MAX_RETRIES_ON_THROTTLE = 5\n",
    "\n",
    "# Tool configuration for Bedrock Converse API\n",
    "# Forces the model to return structured JSON with specific schema, and use calculator tool\n",
    "TOOL_CONFIG = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"judged_scenario_json\",\n",
    "                \"description\": \"Return judged compliance scenarios as JSON\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"scenarios\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"judged-compliant\": {\"type\": \"boolean\"},\n",
    "                                        \"judged-compliant-reason\": {\"type\": \"string\"}\n",
    "                                    },\n",
    "                                    \"required\": [\"judged-compliant\", \"judged-compliant-reason\"]\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"scenarios\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"compliance_calculator\",\n",
    "                \"description\": \"Calculate and compare values with time, money, data, and percentage units\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"expression\": {\"type\": \"string\", \"description\": \"Expression like '800ms < 1s' or '4m > 3b'\"}\n",
    "                        },\n",
    "                        \"required\": [\"expression\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "# CALCULATOR_TOOL[\"toolSpec\"] references the calculator tool definition from compliance_calculator.py\n",
    "\n",
    "# Initialize AWS Bedrock clients\n",
    "# bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name=AWS_REGION)  # For knowledge base retrieval\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=AWS_REGION)  # For model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "a72acffd-ba5a-4a18-99da-f41b6f65cec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:01:58.119553Z",
     "iopub.status.busy": "2026-01-26T00:01:58.119150Z",
     "iopub.status.idle": "2026-01-26T00:01:58.124267Z",
     "shell.execute_reply": "2026-01-26T00:01:58.122596Z",
     "shell.execute_reply.started": "2026-01-26T00:01:58.119528Z"
    }
   },
   "outputs": [],
   "source": [
    "def keep_alive():\n",
    "    while True:\n",
    "        time.sleep(300)  # 5 minutes\n",
    "        print(f\"Keep alive thread still running... {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "4f6cb3ad-046d-4bee-982c-00399c7bbc74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:01:58.524123Z",
     "iopub.status.busy": "2026-01-26T00:01:58.523847Z",
     "iopub.status.idle": "2026-01-26T00:01:58.530002Z",
     "shell.execute_reply": "2026-01-26T00:01:58.528623Z",
     "shell.execute_reply.started": "2026-01-26T00:01:58.524099Z"
    }
   },
   "outputs": [],
   "source": [
    "def bedrock_call_with_retry(func: Callable[[], Any], max_retries: int = MAX_RETRIES_ON_THROTTLE, base_delay: int = 2) -> Any:\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'ThrottlingException':\n",
    "                delay = base_delay * (2 ** attempt) # exponential backoff in the event of throttling\n",
    "                print(f\"Rate limit hit, waiting {delay}s...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "    raise Exception(\"Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_state": "idle",
   "id": "1abdea2c-e311-4358-8b11-3f02472ad8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:01:58.897344Z",
     "iopub.status.busy": "2026-01-26T00:01:58.897064Z",
     "iopub.status.idle": "2026-01-26T00:01:58.902467Z",
     "shell.execute_reply": "2026-01-26T00:01:58.901145Z",
     "shell.execute_reply.started": "2026-01-26T00:01:58.897321Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_scenarios_from_s3(input_bucket: str = BUCKET, s3_source_scenarios: str = S3_SOURCE_SCENARIOS, object_name: str = \"scenarios.json\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load scenarios from S3 JSON file.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=input_bucket, Key=s3_source_scenarios+object_name)\n",
    "    json_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    return json_data[\"scenarios\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_state": "idle",
   "id": "fa5e2795-d84a-452f-9dba-a9ede04e259f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:01:59.253928Z",
     "iopub.status.busy": "2026-01-26T00:01:59.253624Z",
     "iopub.status.idle": "2026-01-26T00:01:59.258539Z",
     "shell.execute_reply": "2026-01-26T00:01:59.257195Z",
     "shell.execute_reply.started": "2026-01-26T00:01:59.253905Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_file_to_s3(file_path: Path, output_bucket: str = BUCKET, s3_key: str = None):\n",
    "    \"\"\"\n",
    "    Upload a local file to S3.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file(str(file_path), output_bucket, s3_key)\n",
    "    print(f\"Uploaded {file_path} to s3://{output_bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "execution_state": "idle",
   "id": "2e937950-914d-4f2e-88bb-0d757125f96a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:01:59.575751Z",
     "iopub.status.busy": "2026-01-26T00:01:59.575396Z",
     "iopub.status.idle": "2026-01-26T00:01:59.581017Z",
     "shell.execute_reply": "2026-01-26T00:01:59.579520Z",
     "shell.execute_reply.started": "2026-01-26T00:01:59.575727Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_policies_by_id(bucket:str, folder:str, policy_ids: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve specific policy documents from s3.\n",
    "    \"\"\"\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    policies = []\n",
    "    for policy_id in policy_ids:\n",
    "        response = s3.get_object(Bucket=bucket, Key=folder + policy_id + \".md\")\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        policies.append(f\"{policy_id}:\\n{content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(policies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "execution_state": "idle",
   "id": "64274281-02c9-467a-8ab9-e959c950f919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:01:59.971958Z",
     "iopub.status.busy": "2026-01-26T00:01:59.971656Z",
     "iopub.status.idle": "2026-01-26T00:01:59.979607Z",
     "shell.execute_reply": "2026-01-26T00:01:59.977761Z",
     "shell.execute_reply.started": "2026-01-26T00:01:59.971933Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_policies_for_scenario(scenario: Dict) -> str:\n",
    "    \"\"\"Extract policy IDs from scenario and retrieve policy content from S3\"\"\"\n",
    "    \n",
    "    # Extract policy IDs from scenario text\n",
    "    policy_match = re.search(r'Policies referenced: (.+)', scenario[\"scenario-detail\"])\n",
    "    if not policy_match:\n",
    "        print(f\"No policies referenced in scenario: {scenario['scenario-detail']}\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Parse policy IDs\n",
    "    policy_ids = [p.strip() for p in policy_match.group(1).split(',')]\n",
    "    \n",
    "    # Retrieve policy documents from S3\n",
    "    s3 = boto3.client('s3')\n",
    "    policies = []\n",
    "    for policy_id in policy_ids:\n",
    "        response = s3.get_object(Bucket=BUCKET, Key=S3_SOURCE_POLICY_ALL + policy_id + \".md\")\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        policies.append(f\"{policy_id}:\\n{content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(policies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "execution_state": "idle",
   "id": "464020d0-7465-4541-84c3-55620b1f78c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:02:00.433246Z",
     "iopub.status.busy": "2026-01-26T00:02:00.432849Z",
     "iopub.status.idle": "2026-01-26T00:02:00.443656Z",
     "shell.execute_reply": "2026-01-26T00:02:00.442026Z",
     "shell.execute_reply.started": "2026-01-26T00:02:00.433214Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_with_claude(\n",
    "    prompt: str,\n",
    "    model_id: str,\n",
    "    temperature: float\n",
    ") -> Dict:\n",
    "    \n",
    "    messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    result = {}\n",
    "    \n",
    "    while True:\n",
    "        response = bedrock_call_with_retry(\n",
    "            lambda: bedrock_runtime.converse(modelId=model_id, messages=messages,toolConfig=TOOL_CONFIG,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": MAX_TOKENS,\n",
    "                    \"temperature\": temperature \n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # track per-scenario token usage\n",
    "        usage = response.get('usage', {})\n",
    "        input_tokens += usage.get('inputTokens', 0)\n",
    "        output_tokens += usage.get('outputTokens', 0)\n",
    "        \n",
    "        if response['stopReason'] == 'tool_use':\n",
    "            tool_results = []\n",
    "            for content_block in response['output']['message']['content']:\n",
    "                if 'toolUse' in content_block:\n",
    "                    tool_name = content_block['toolUse']['name']\n",
    "                    tool_use_id = content_block['toolUse']['toolUseId']\n",
    "                    \n",
    "                    if tool_name == 'compliance_calculator':\n",
    "                        expression = content_block['toolUse']['input']['expression']                         \n",
    "                        calc_result = compliance_calculator(expression)\n",
    "                        # print(\"=\" * 60)\n",
    "                        # print(f\"Compliance calculator expression: {expression}\")\n",
    "                        # print(f\"Compliance calculator result: {calc_result}\")\n",
    "                        # print(\"=\" * 60)\n",
    "                        tool_results.append({\n",
    "                            \"toolResult\": {\n",
    "                                \"toolUseId\": tool_use_id,\n",
    "                                \"content\": [{\"text\": calc_result}]\n",
    "                            }\n",
    "                        })\n",
    "                    elif tool_name == 'judged_scenario_json':\n",
    "                        tool_result = content_block['toolUse']['input']\n",
    "                         # Parse JSON if it's a string\n",
    "                        if isinstance(tool_result, str):\n",
    "                            tool_result = json.loads(tool_result)\n",
    "                        result[\"judged-compliant\"] = tool_result['scenarios'][0]['judged-compliant']\n",
    "                        result[\"judged-compliant-reason\"] = tool_result['scenarios'][0]['judged-compliant-reason']\n",
    "                        break\n",
    "            \n",
    "            if tool_results:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": response['output']['message']['content']})\n",
    "                messages.append({\"role\": \"user\", \"content\": tool_results})\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    result.update({\n",
    "        \"llm-judge-input-tokens\" : input_tokens,\n",
    "        \"llm-judge-output-tokens\": output_tokens,\n",
    "        \"llm-judge-total-tokens\" : input_tokens + output_tokens\n",
    "    })\n",
    "        \n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "execution_state": "idle",
   "id": "e5889b17-5c28-4da7-a7a5-10c5c07a88d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:02:01.084019Z",
     "iopub.status.busy": "2026-01-26T00:02:01.083651Z",
     "iopub.status.idle": "2026-01-26T00:02:01.095868Z",
     "shell.execute_reply": "2026-01-26T00:02:01.091127Z",
     "shell.execute_reply.started": "2026-01-26T00:02:01.083990Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_with_nova(prompt: str, model_id: str, temperature: float) -> Dict:\n",
    "    enhanced_prompt = f\"\"\"{prompt}\n",
    "\n",
    "Respond with a JSON object in this exact format:\n",
    "{{\n",
    "  \"judged-compliant\": true/false,\n",
    "  \"judged-compliant-reason\": \"detailed explanation\"\n",
    "}}\"\"\"\n",
    "    \n",
    "    body = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": enhanced_prompt}]}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": MAX_TOKENS,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = bedrock_call_with_retry(\n",
    "        lambda: bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    content = response_body['output']['message']['content'][0]['text']\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    try:\n",
    "        # Find JSON in response text\n",
    "        start = content.find('{')\n",
    "        end = content.rfind('}') + 1\n",
    "        json_str = content[start:end]\n",
    "        result = json.loads(json_str)\n",
    "        \n",
    "        return {\n",
    "            \"judged-compliant\": result[\"judged-compliant\"],\n",
    "            \"judged-compliant-reason\": result[\"judged-compliant-reason\"],\n",
    "            \"llm-judge-input-tokens\": response_body.get('usage', {}).get('inputTokens', 0),\n",
    "            \"llm-judge-output-tokens\": response_body.get('usage', {}).get('outputTokens', 0),\n",
    "            \"llm-judge-total-tokens\": response_body.get('usage', {}).get('inputTokens', 0) + response_body.get('usage', {}).get('outputTokens', 0)\n",
    "        }\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        return {\n",
    "            \"judged-compliant\": False,\n",
    "            \"judged-compliant-reason\": f\"Error parsing Nova response: {str(e)}\",\n",
    "            \"llm-judge-input-tokens\": 0,\n",
    "            \"llm-judge-output-tokens\": 0,\n",
    "            \"llm-judge-total-tokens\": 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "execution_state": "idle",
   "id": "01b553aa-0099-445e-a80e-afed0fcbef57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:02:01.611468Z",
     "iopub.status.busy": "2026-01-26T00:02:01.611018Z",
     "iopub.status.idle": "2026-01-26T00:02:01.622831Z",
     "shell.execute_reply": "2026-01-26T00:02:01.619720Z",
     "shell.execute_reply.started": "2026-01-26T00:02:01.611398Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_scenarios(\n",
    "    source_scenarios: List[Dict],\n",
    "    model_arn: str, \n",
    "    temperature: float = None,\n",
    "    max_scenarios: int = None\n",
    ") -> List[Dict]:\n",
    "    \n",
    "    # Extract model ID and detect model type\n",
    "    model_id = model_arn.split('/')[-1] if '/' in model_arn else model_arn\n",
    "    is_nova_model = \"nova\" in model_id.lower()\n",
    "    \n",
    "    judged_scenarios = []\n",
    "    for scenario in source_scenarios[:max_scenarios] if max_scenarios else source_scenarios:\n",
    "        \n",
    "        # Get policy context from S3 for this scenario\n",
    "        retrieved_policies = get_policies_for_scenario(scenario)\n",
    "        if not retrieved_policies:\n",
    "            continue  # Skip scenarios without policy references\n",
    "        \n",
    "        prompt = COMPLIANCE_JUDGE_PROMPT.format(\n",
    "            retrieved_policies=retrieved_policies,\n",
    "            scenario_detail=scenario[\"scenario-detail\"]\n",
    "        )\n",
    "        \n",
    "        if is_nova_model:\n",
    "            # Nova: Use invoke_model with JSON response\n",
    "            judged_scenario_detail = judge_with_nova(prompt, model_id, temperature)\n",
    "            print(\"Calling judge_with_nova\")\n",
    "        else:\n",
    "            # Claude: Use converse with tools\n",
    "            judged_scenario_detail = judge_with_claude(prompt, model_id, temperature)\n",
    "            print(\"Calling judge_with_claude\")\n",
    "        \n",
    "        # Add metadata\n",
    "        judged_scenario = scenario.copy()\n",
    "        judged_scenario.update(judged_scenario_detail)\n",
    "        judged_scenario.update({\n",
    "            \"judged-dtm\": datetime.datetime.now().isoformat(),\n",
    "            \"llm-judge\": model_id,\n",
    "            \"llm-judge-temp\": temperature\n",
    "        })\n",
    "        \n",
    "        judged_scenarios.append(judged_scenario)\n",
    "    \n",
    "    return judged_scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "execution_state": "idle",
   "id": "aaf8bcc2-d0f7-40bb-8c04-17f9981de4fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:02:02.201617Z",
     "iopub.status.busy": "2026-01-26T00:02:02.201345Z",
     "iopub.status.idle": "2026-01-26T00:02:02.209972Z",
     "shell.execute_reply": "2026-01-26T00:02:02.208706Z",
     "shell.execute_reply.started": "2026-01-26T00:02:02.201596Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_scenarios_streaming(\n",
    "    source_scenarios: List[Dict],\n",
    "    model_arn: str, \n",
    "    temperature: float = None,\n",
    "    max_scenarios: int = None,\n",
    "    output_file: Path = None\n",
    ") -> List[Dict]:\n",
    "    \n",
    "    model_id = model_arn.split('/')[-1] if '/' in model_arn else model_arn\n",
    "    is_nova_model = \"nova\" in model_id.lower()\n",
    "    \n",
    "    # Initialize file with opening bracket\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('{\"scenarios\":[')\n",
    "    \n",
    "    judged_scenarios = []\n",
    "    scenarios_to_process = source_scenarios[:max_scenarios] if max_scenarios else source_scenarios\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios_to_process):\n",
    "        retrieved_policies = get_policies_for_scenario(scenario)\n",
    "        if not retrieved_policies:\n",
    "            continue\n",
    "        \n",
    "        prompt = COMPLIANCE_JUDGE_PROMPT.format(\n",
    "            retrieved_policies=retrieved_policies,\n",
    "            scenario_detail=scenario[\"scenario-detail\"]\n",
    "        )\n",
    "        \n",
    "        if is_nova_model:\n",
    "            judged_scenario_detail = judge_with_nova(prompt, model_id, temperature)\n",
    "        else:\n",
    "            judged_scenario_detail = judge_with_claude(prompt, model_id, temperature)\n",
    "        \n",
    "        judged_scenario = scenario.copy()\n",
    "        judged_scenario.update(judged_scenario_detail)\n",
    "        judged_scenario.update({\n",
    "            \"judged-dtm\": datetime.datetime.now().isoformat(),\n",
    "            \"llm-judge\": model_id,\n",
    "            \"llm-judge-temp\": temperature\n",
    "        })\n",
    "        \n",
    "        judged_scenarios.append(judged_scenario)\n",
    "        \n",
    "        # Append to file immediately\n",
    "        with open(output_file, 'a') as f:\n",
    "            if i > 0:  # Add comma before all but first scenario\n",
    "                f.write(',')\n",
    "            json.dump(judged_scenario, f)\n",
    "        \n",
    "        print(f\"Saved scenario {i+1}/{len(scenarios_to_process)}\")\n",
    "    \n",
    "    # Close the JSON structure\n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(']}')\n",
    "    \n",
    "    return judged_scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "execution_state": "idle",
   "id": "b17b51d7-4d3c-4258-bd1b-58e80adb8763",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:02:03.175803Z",
     "iopub.status.busy": "2026-01-26T00:02:03.175530Z",
     "iopub.status.idle": "2026-01-26T00:02:03.181866Z",
     "shell.execute_reply": "2026-01-26T00:02:03.180413Z",
     "shell.execute_reply.started": "2026-01-26T00:02:03.175783Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_scenarios_to_file(scenarios: List[Dict], output_path: Path):\n",
    "    \n",
    "    # Print scenarios to console for immediate review\n",
    "    print(json.dumps(scenarios, indent=2))\n",
    "\n",
    "    # Create parent directories if they don't exist\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save to file with metadata and statistics\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'total_scenarios': len(scenarios),\n",
    "            'compliant_count': sum(1 for s in scenarios if s['is-compliant']),\n",
    "            'non_compliant_count': sum(1 for s in scenarios if not s['is-compliant']),\n",
    "            'judged_compliant_count': sum(1 for s in scenarios if s['judged-compliant']),\n",
    "            'judged_non_compliant_count': sum(1 for s in scenarios if not s['judged-compliant']),\n",
    "            'scenarios': scenarios\n",
    "        }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "execution_state": "idle",
   "id": "8b5a5088-bba4-4786-82b1-44374c091b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T00:02:04.752016Z",
     "iopub.status.busy": "2026-01-26T00:02:04.751688Z",
     "iopub.status.idle": "2026-01-26T00:02:04.760128Z",
     "shell.execute_reply": "2026-01-26T00:02:04.758514Z",
     "shell.execute_reply.started": "2026-01-26T00:02:04.751986Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Start keep-alive thread\n",
    "    # daemon=True parameter makes the thread automatically stop when the main program ends\n",
    "    thread = threading.Thread(target=keep_alive, daemon=True)\n",
    "    thread.start()\n",
    "    \n",
    "    judger_models = [\n",
    "        {\n",
    "            'name': 'claude_3_7_sonnet',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
    "            'temperature': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'claude_4_sonnet',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0',\n",
    "            'temperature': 0.0\n",
    "        },\n",
    "        {\n",
    "            'name': 'claude_opus_4_5',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/global.anthropic.claude-opus-4-5-20251101-v1:0',\n",
    "            'temperature': 0.0\n",
    "        },\n",
    "        {\n",
    "            'name': 'nova_premier',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-premier-v1:0',\n",
    "            'temperature': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'nova_2_lite',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-2-lite-v1:0',\n",
    "            'temperature': 0.2\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    source_scenarios_files = [\"scenarios-8-policies-each.json\", \"scenarios-6-policies-each.json\"]#, \"scenarios-4-policies-each.json\"]\n",
    "    for source_scenarios_file in source_scenarios_files:\n",
    "     \n",
    "        source_scenarios = load_scenarios_from_s3(BUCKET, S3_SOURCE_SCENARIOS, source_scenarios_file)\n",
    "\n",
    "        for model in judger_models:\n",
    "            try:    \n",
    "                judged_scenarios_batch = f\"judged_scenarios_batch-{source_scenarios_file.split('.')[0]}-{model['name']}-temp{model['temperature']}\"\n",
    "                print (\"=\" * 100)\n",
    "                print(f\"Starting batch: {judged_scenarios_batch}\")\n",
    "                print (\"=\" * 100)\n",
    "\n",
    "                judged_scenarios_file = f\"{judged_scenarios_batch}.json\"\n",
    "                local_file_path: Path = FOLDER_JUDGED_SCENARIOS / judged_scenarios_file\n",
    "                s3_key = S3_JUDGED_SCENARIOS + judged_scenarios_file\n",
    "                \n",
    "                judged_scenarios = judge_scenarios_streaming(\n",
    "                    source_scenarios = source_scenarios,\n",
    "                    model_arn = model['arn'],\n",
    "                    temperature = model['temperature'],\n",
    "                    max_scenarios = 200,\n",
    "                    output_file = local_file_path\n",
    "                )\n",
    "\n",
    "                save_file_to_s3(local_file_path, BUCKET, s3_key)\n",
    "                \n",
    "                print (\"=\" * 100)\n",
    "                print(f\"Finished batch: {judged_scenarios_batch}: {len(judged_scenarios)} scenarios processed\")\n",
    "                print (\"=\" * 100)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {judged_scenarios_batch}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()  # This will show the full error trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "idle",
   "id": "c0fae162-5344-421d-9ab6-29dfef33dcdb",
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-26T00:02:59.180Z",
     "iopub.execute_input": "2026-01-26T00:02:06.368373Z",
     "iopub.status.busy": "2026-01-26T00:02:06.368065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Starting batch: judged_scenarios_batch-scenarios-8-policies-each-claude_3_7_sonnet-temp0.1\n",
      "====================================================================================================\n",
      "Saved scenario 1/2\n",
      "Saved scenario 2/2\n",
      "Uploaded /home/sagemaker-user/data/judged_scenarios/judged_scenarios_batch-scenarios-8-policies-each-claude_3_7_sonnet-temp0.1.json to s3://183023889407-us-east-1-compliance-rule-generator/scenarios-judged/judged_scenarios_batch-scenarios-8-policies-each-claude_3_7_sonnet-temp0.1.json\n",
      "====================================================================================================\n",
      "Finished batch: judged_scenarios_batch-scenarios-8-policies-each-claude_3_7_sonnet-temp0.1: 2 scenarios processed\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Starting batch: judged_scenarios_batch-scenarios-8-policies-each-claude_4_sonnet-temp0.0\n",
      "====================================================================================================\n",
      "Saved scenario 1/2\n",
      "Saved scenario 2/2\n",
      "Uploaded /home/sagemaker-user/data/judged_scenarios/judged_scenarios_batch-scenarios-8-policies-each-claude_4_sonnet-temp0.0.json to s3://183023889407-us-east-1-compliance-rule-generator/scenarios-judged/judged_scenarios_batch-scenarios-8-policies-each-claude_4_sonnet-temp0.0.json\n",
      "====================================================================================================\n",
      "Finished batch: judged_scenarios_batch-scenarios-8-policies-each-claude_4_sonnet-temp0.0: 2 scenarios processed\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Starting batch: judged_scenarios_batch-scenarios-8-policies-each-claude_opus_4_5-temp0.0\n",
      "====================================================================================================\n",
      "Saved scenario 1/2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "execution_state": "idle",
   "id": "61d791d7-018d-4391-b2b6-b5242d108ca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T21:55:53.423741Z",
     "iopub.status.busy": "2026-01-25T21:55:53.423463Z",
     "iopub.status.idle": "2026-01-25T21:55:53.687029Z",
     "shell.execute_reply": "2026-01-25T21:55:53.685839Z",
     "shell.execute_reply.started": "2026-01-25T21:55:53.423721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-2-11b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-2-3b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-2-90b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-2-1b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-1-8b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-1-70b-instruct-v1:0\n",
      "ARN: arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.meta.llama3-3-70b-instruct-v1:0\n"
     ]
    }
   ],
   "source": [
    "bedrock_client = boto3.client('bedrock', region_name=AWS_REGION)\n",
    "response = bedrock_client.list_inference_profiles()\n",
    "for profile in response['inferenceProfileSummaries']:\n",
    "    if 'meta' in profile['inferenceProfileName'].lower():\n",
    "        print(f\"ARN: {profile['inferenceProfileArn']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736c07d-3bca-4aca-848e-3e66a07f9896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
