{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae2603e-4fea-446e-9216-700fc15171c2",
   "metadata": {},
   "source": [
    "# NIST SP 800-53 Controls Processing Pipeline\n",
    "## Step 4: Judge Scenarios\n",
    "\n",
    "**Purpose**: Evaluate LLM performance on NIST compliance scenarios by comparing model judgments against ground truth labels to assess accuracy and price/performance characteristics across different models.\n",
    "\n",
    "**Author**: NIST Compliance LLM Evaluation Framework  \n",
    "**Last Updated**: January 2025  \n",
    "**Environment**: SageMaker Studio, Python 3.11\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook serves as the fourth and final stage in a four-part pipeline for evaluating LLM performance on NIST compliance scenarios. It processes compliance scenarios generated in Step 3, evaluating various LLMs' ability to correctly assess compliance against organizational policies derived from NIST controls.\n",
    "\n",
    "### Business Context\n",
    "\n",
    "Organizations implementing NIST cybersecurity frameworks need reliable automated tools to assess compliance. This pipeline enables systematic evaluation of different LLM models' accuracy, consistency, and cost-effectiveness for compliance automation tasks, supporting data-driven decisions for regulatory technology investments.\n",
    "\n",
    "### Data\n",
    "\n",
    "#### Input\n",
    "- Labeled compliance scenarios from S3 (JSON format with ground truth labels)\n",
    "- Policy documents retrieved from S3 per scenario\n",
    "#### Output\n",
    " - Model evaluation results with accuracy and token consumption stored in S3\n",
    "\n",
    "### Input JSON Structure\n",
    "\n",
    "Scenarios loaded from S3 with the following structure:\n",
    "```json\n",
    "{\n",
    "  \"scenarios\": [\n",
    "    {\n",
    "      \"scenario-id\": \"scenario-id-1\",\n",
    "      \"scenario-detail\": \"A new employee, Sarah Johnson, joins the IT department... Policies referenced: POL_AC-2, POL_IA-4\",\n",
    "      \"is-compliant\": false,\n",
    "      \"non-compliant-reason\": \"The scenario violates access control policy...\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Output JSON Structure\n",
    "\n",
    "Judged scenarios with LLM evaluation results:\n",
    "```json\n",
    "{\n",
    "  \"scenarios\": [\n",
    "    {\n",
    "      \"scenario-id\": \"scenario-id-1\",\n",
    "      \"scenario-detail\": \"A new employee, Sarah Johnson, joins the IT department...\",\n",
    "      \"is-compliant\": false,\n",
    "      \"non-compliant-reason\": \"The scenario violates...\",\n",
    "      \"judged-compliant\": true,\n",
    "      \"judged-compliant-reason\": \"Considered the rules AC... and scenario is not in violation...\",\n",
    "      \"llm-judge\": \"global.anthropic.claude-opus-4-5-20251101-v1:0\",\n",
    "      \"llm-judge-temp\": 0.1,\n",
    "      \"llm-judge-input-tokens\": 1250,\n",
    "      \"llm-judge-output-tokens\": 180,\n",
    "      \"llm-judge-total-tokens\": 1430,\n",
    "      \"judged-dtm\": \"2026-02-08T23:15:17.215554\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "### Processing Steps\n",
    "\n",
    "1. Retrieves labeled compliance test scenarios from S3 storage (`scenarios/` prefix)\n",
    "2. Parses \"Policies referenced:\" from scenario text to identify relevant policies\n",
    "3. Loads specific policy documents from S3 (`policies/markdown/all-policies-main/` prefix)\n",
    "4. Tests LLM models against scenarios using AWS Bedrock with structured JSON output\n",
    "5. Records input/output tokens and total costs per scenario evaluation\n",
    "6. Saves evaluation results to S3 (`scenarios-judged/` prefix) with streaming output\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- Evaluates Claude (Converse API with tools) and Nova (invoke_model with JSON parsing)\n",
    "- Extracts policy IDs from scenario text and retrieves full policy content from S3\n",
    "- Forces Claude models to return JSON with compliance reasoning via tool configuration (not supported by Nova)\n",
    "- Tracks token usage (input/output/total) and processing time per scenario\n",
    "- Saves results incrementally to prevent data loss during long-running evaluations\n",
    "- Exponential backoff retry logic for API throttling (max 5 retries)\n",
    "- Background thread prevents session timeout during long evaluations\n",
    "\n",
    "### Configuration Parameters\n",
    "\n",
    "- Token Limits: 4096 max tokens per Bedrock call\n",
    "- 5 max retries with exponential backoff (2^attempt seconds)\n",
    "\n",
    "### Tool Configuration\n",
    "\n",
    "For Claude only (not supported by Nova), the notebook uses Bedrock Converse API with two tools:\n",
    "1. Forces structured JSON output with `judged-compliant` (boolean) and `judged-compliant-reason` (string)\n",
    "2. compliance_calculator handles time/money/data unit comparisons in policy evaluation\n",
    "\n",
    "### Output Artifacts\n",
    "\n",
    "- JSON files with model judgments, token usage, and timestamps\n",
    "- Per-scenario token counts for price/performance analysis\n",
    "- Files named `judged_scenarios_batch-{scenario_file}-{model_name}-temp{temperature}.json`\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "- AWS Bedrock access with model permissions (Claude, Nova)\n",
    "- AWS S3 read/write access to compliance bucket\n",
    "- Python libraries: boto3, datetime, json, re, time, threading, pathlib\n",
    "- compliance_utils module (COMPLIANCE_JUDGE_PROMPT, compliance_calculator)\n",
    "- Compliance scenarios from Step 3 and policy documents from Step 2\n",
    "\n",
    "### Architecture Notes\n",
    "\n",
    "- Uses direct S3 policy retrieval instead of Knowledge Base for 100% accuracy\n",
    "- Supports both Claude (tool-based) and Nova (JSON parsing) model types\n",
    "- Implements streaming file output to handle large scenario batches\n",
    "- Includes error handling and progress tracking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "721c6452-39b4-4d10-875f-87c1fb55f2df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:04.795378Z",
     "iopub.status.busy": "2026-02-16T03:07:04.795110Z",
     "iopub.status.idle": "2026-02-16T03:07:04.948808Z",
     "shell.execute_reply": "2026-02-16T03:07:04.947483Z",
     "shell.execute_reply.started": "2026-02-16T03:07:04.795352Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import time   # For rate limiting between API calls\n",
    "import threading\n",
    "from botocore.exceptions import ClientError\n",
    "from compliance_utils import compliance_calculator, CALCULATOR_TOOL, COMPLIANCE_JUDGE_PROMPT\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Callable, Any\n",
    "\n",
    "FOLDER_HOME: Path = Path('/home/sagemaker-user')\n",
    "FOLDER_JUDGED_SCENARIOS: Path = FOLDER_HOME / 'data/judged_scenarios/'\n",
    "BUCKET = '183023889407-us-east-1-compliance-rule-generator'\n",
    "S3_SOURCE_SCENARIOS = 'scenarios/'  # Folder path in S3 where scenarios are stored\n",
    "S3_SOURCE_POLICY_ALL = 'policies/markdown/all-policies-main/'\n",
    "S3_JUDGED_SCENARIOS = 'scenarios-judged/'  # Folder path for results\n",
    "AWS_REGION = 'us-east-1'\n",
    "# KNOWLEDGE_BASE_ID = 'T8EW10IU3Z' - using s3 to retrieve policies, KB performance unsatisfactory\n",
    "MAX_TOKENS = 4096\n",
    "MAX_RETRIES_ON_THROTTLE = 5\n",
    "\n",
    "# Tool configuration for Bedrock Converse API\n",
    "# Forces the model to return structured JSON with specific schema, and use calculator tool\n",
    "TOOL_CONFIG = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"judged_scenario_json\",\n",
    "                \"description\": \"Return judged compliance scenarios as JSON\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"scenarios\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"object\",\n",
    "                                    \"properties\": {\n",
    "                                        \"judged-compliant\": {\"type\": \"boolean\"},\n",
    "                                        \"judged-compliant-reason\": {\"type\": \"string\"}\n",
    "                                    },\n",
    "                                    \"required\": [\"judged-compliant\", \"judged-compliant-reason\"]\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"scenarios\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"compliance_calculator\",\n",
    "                \"description\": \"Calculate and compare values with time, money, data, and percentage units\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"expression\": {\"type\": \"string\", \"description\": \"Expression like '800ms < 1s' or '4m > 3b'\"}\n",
    "                        },\n",
    "                        \"required\": [\"expression\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "MODELS = [\n",
    "        {\n",
    "            'name': 'claude_3_7_sonnet-t0.0',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
    "            'temperature': 0.0\n",
    "        },\n",
    "        {\n",
    "            'name': 'claude_3_7_sonnett-t0.1',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
    "            'temperature': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'claude_4_sonnet-t0.0',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0',\n",
    "            'temperature': 0.0\n",
    "        },\n",
    "        {\n",
    "            'name': 'claude_4_sonnet-t0.1',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0',\n",
    "            'temperature': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'claude_opus_4_5-t0.0',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/global.anthropic.claude-opus-4-5-20251101-v1:0',\n",
    "            'temperature': 0.0\n",
    "        },\n",
    "        {\n",
    "            'name': 'claude_opus_4_5-t0.1',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/global.anthropic.claude-opus-4-5-20251101-v1:0',\n",
    "            'temperature': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'nova_premier-t0.0',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-premier-v1:0',\n",
    "            'temperature': 0.0\n",
    "        },\n",
    "        {\n",
    "            'name': 'nova_premier-t0.1',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-premier-v1:0',\n",
    "            'temperature': 0.1\n",
    "        },\n",
    "        {\n",
    "            'name': 'nova_2_lite-t0.0',\n",
    "            'arn': 'arn:aws:bedrock:us-east-1:183023889407:inference-profile/us.amazon.nova-2-lite-v1:0',\n",
    "            'temperature': 0.0\n",
    "        }\n",
    "    ]\n",
    "\n",
    "SCENARIOS = [\"scenarios-10-policies-each-200.json\", \"scenarios-8-policies-each.json\", \"scenarios-6-policies-each.json\", \"scenarios-4-policies-each.json\"]\n",
    "\n",
    "# CALCULATOR_TOOL[\"toolSpec\"] references the calculator tool definition from compliance_calculator.py\n",
    "\n",
    "# Initialize AWS Bedrock clients\n",
    "# bedrock_agent_runtime = boto3.client('bedrock-agent-runtime', region_name=AWS_REGION)  # For knowledge base retrieval\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=AWS_REGION)  # For model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "a72acffd-ba5a-4a18-99da-f41b6f65cec1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:05.184106Z",
     "iopub.status.busy": "2026-02-16T03:07:05.183823Z",
     "iopub.status.idle": "2026-02-16T03:07:05.193976Z",
     "shell.execute_reply": "2026-02-16T03:07:05.189696Z",
     "shell.execute_reply.started": "2026-02-16T03:07:05.184079Z"
    }
   },
   "outputs": [],
   "source": [
    "def keep_alive():\n",
    "    while True:\n",
    "        time.sleep(300)  # 5 minutes\n",
    "        print(f\"Keep alive thread still running... {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "4f6cb3ad-046d-4bee-982c-00399c7bbc74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:05.503526Z",
     "iopub.status.busy": "2026-02-16T03:07:05.498200Z",
     "iopub.status.idle": "2026-02-16T03:07:05.520410Z",
     "shell.execute_reply": "2026-02-16T03:07:05.516990Z",
     "shell.execute_reply.started": "2026-02-16T03:07:05.503466Z"
    }
   },
   "outputs": [],
   "source": [
    "def bedrock_call_with_retry(func: Callable[[], Any], max_retries: int = MAX_RETRIES_ON_THROTTLE, base_delay: int = 2) -> Any:\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except ClientError as e:\n",
    "            if e.response['Error']['Code'] == 'ThrottlingException':\n",
    "                delay = base_delay * (2 ** attempt) # exponential backoff in the event of throttling\n",
    "                print(f\"Rate limit hit, waiting {delay}s...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "    raise Exception(\"Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "1abdea2c-e311-4358-8b11-3f02472ad8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:05.724419Z",
     "iopub.status.busy": "2026-02-16T03:07:05.724097Z",
     "iopub.status.idle": "2026-02-16T03:07:05.742683Z",
     "shell.execute_reply": "2026-02-16T03:07:05.740700Z",
     "shell.execute_reply.started": "2026-02-16T03:07:05.724388Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_scenarios_from_s3(input_bucket: str = BUCKET, s3_source_scenarios: str = S3_SOURCE_SCENARIOS, object_name: str = \"scenarios.json\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load scenarios from S3 JSON file.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.get_object(Bucket=input_bucket, Key=s3_source_scenarios+object_name)\n",
    "    json_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    return json_data[\"scenarios\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_state": "idle",
   "id": "fa5e2795-d84a-452f-9dba-a9ede04e259f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:05.920303Z",
     "iopub.status.busy": "2026-02-16T03:07:05.919882Z",
     "iopub.status.idle": "2026-02-16T03:07:05.931220Z",
     "shell.execute_reply": "2026-02-16T03:07:05.929402Z",
     "shell.execute_reply.started": "2026-02-16T03:07:05.920268Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_file_to_s3(file_path: Path, output_bucket: str = BUCKET, s3_key: str = None):\n",
    "    \"\"\"\n",
    "    Upload a local file to S3.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file(str(file_path), output_bucket, s3_key)\n",
    "    print(f\"Uploaded {file_path} to s3://{output_bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_state": "idle",
   "id": "2e937950-914d-4f2e-88bb-0d757125f96a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:06.089020Z",
     "iopub.status.busy": "2026-02-16T03:07:06.088520Z",
     "iopub.status.idle": "2026-02-16T03:07:06.100870Z",
     "shell.execute_reply": "2026-02-16T03:07:06.097100Z",
     "shell.execute_reply.started": "2026-02-16T03:07:06.088982Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_policies_by_id(bucket:str, folder:str, policy_ids: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve specific policy documents from s3.\n",
    "    \"\"\"\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    policies = []\n",
    "    for policy_id in policy_ids:\n",
    "        response = s3.get_object(Bucket=bucket, Key=folder + policy_id + \".md\")\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        policies.append(f\"{policy_id}:\\n{content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(policies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "execution_state": "idle",
   "id": "64274281-02c9-467a-8ab9-e959c950f919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:06.427377Z",
     "iopub.status.busy": "2026-02-16T03:07:06.426269Z",
     "iopub.status.idle": "2026-02-16T03:07:06.450548Z",
     "shell.execute_reply": "2026-02-16T03:07:06.448370Z",
     "shell.execute_reply.started": "2026-02-16T03:07:06.427339Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_policies_for_scenario(scenario: Dict) -> str:\n",
    "    \"\"\"Extract policy IDs from scenario and retrieve policy content from S3\"\"\"\n",
    "    \n",
    "    # Extract policy IDs from scenario text\n",
    "    policy_match = re.search(r'Policies referenced: (.+)', scenario[\"scenario-detail\"])\n",
    "    if not policy_match:\n",
    "        print(f\"No policies referenced in scenario: {scenario['scenario-detail']}\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Parse policy IDs\n",
    "    policy_ids = [p.strip() for p in policy_match.group(1).split(',')]\n",
    "    \n",
    "    # Retrieve policy documents from S3\n",
    "    s3 = boto3.client('s3')\n",
    "    policies = []\n",
    "    for policy_id in policy_ids:\n",
    "        response = s3.get_object(Bucket=BUCKET, Key=S3_SOURCE_POLICY_ALL + policy_id + \".md\")\n",
    "        content = response['Body'].read().decode('utf-8')\n",
    "        policies.append(f\"{policy_id}:\\n{content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(policies)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "execution_state": "idle",
   "id": "464020d0-7465-4541-84c3-55620b1f78c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:06.761729Z",
     "iopub.status.busy": "2026-02-16T03:07:06.761438Z",
     "iopub.status.idle": "2026-02-16T03:07:06.787989Z",
     "shell.execute_reply": "2026-02-16T03:07:06.780781Z",
     "shell.execute_reply.started": "2026-02-16T03:07:06.761701Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_with_claude(\n",
    "    prompt: str,\n",
    "    model_id: str,\n",
    "    temperature: float\n",
    ") -> Dict:\n",
    "    \n",
    "    messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    result = {}\n",
    "    \n",
    "    while True:\n",
    "        response = bedrock_call_with_retry(\n",
    "            lambda: bedrock_runtime.converse(modelId=model_id, messages=messages,toolConfig=TOOL_CONFIG,\n",
    "                inferenceConfig={\n",
    "                    \"maxTokens\": MAX_TOKENS,\n",
    "                    \"temperature\": temperature \n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # track per-scenario token usage\n",
    "        usage = response.get('usage', {})\n",
    "        input_tokens += usage.get('inputTokens', 0)\n",
    "        output_tokens += usage.get('outputTokens', 0)\n",
    "        \n",
    "        if response['stopReason'] == 'tool_use':\n",
    "            tool_results = []\n",
    "            for content_block in response['output']['message']['content']:\n",
    "                if 'toolUse' in content_block:\n",
    "                    tool_name = content_block['toolUse']['name']\n",
    "                    tool_use_id = content_block['toolUse']['toolUseId']\n",
    "                    \n",
    "                    if tool_name == 'compliance_calculator':\n",
    "                        expression = content_block['toolUse']['input']['expression']                         \n",
    "                        calc_result = compliance_calculator(expression)\n",
    "                        # print(\"=\" * 60)\n",
    "                        # print(f\"Compliance calculator expression: {expression}\")\n",
    "                        # print(f\"Compliance calculator result: {calc_result}\")\n",
    "                        # print(\"=\" * 60)\n",
    "                        tool_results.append({\n",
    "                            \"toolResult\": {\n",
    "                                \"toolUseId\": tool_use_id,\n",
    "                                \"content\": [{\"text\": calc_result}]\n",
    "                            }\n",
    "                        })\n",
    "                    elif tool_name == 'judged_scenario_json':\n",
    "                        tool_result = content_block['toolUse']['input']\n",
    "                         # Parse JSON if it's a string\n",
    "                        if isinstance(tool_result, str):\n",
    "                            tool_result = json.loads(tool_result)\n",
    "                        result[\"judged-compliant\"] = tool_result['scenarios'][0]['judged-compliant']\n",
    "                        result[\"judged-compliant-reason\"] = tool_result['scenarios'][0]['judged-compliant-reason']\n",
    "                        break\n",
    "            \n",
    "            if tool_results:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": response['output']['message']['content']})\n",
    "                messages.append({\"role\": \"user\", \"content\": tool_results})\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    result.update({\n",
    "        \"llm-judge-input-tokens\" : input_tokens,\n",
    "        \"llm-judge-output-tokens\": output_tokens,\n",
    "        \"llm-judge-total-tokens\" : input_tokens + output_tokens\n",
    "    })\n",
    "        \n",
    "    return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "execution_state": "idle",
   "id": "e5889b17-5c28-4da7-a7a5-10c5c07a88d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:07.517723Z",
     "iopub.status.busy": "2026-02-16T03:07:07.517427Z",
     "iopub.status.idle": "2026-02-16T03:07:07.531724Z",
     "shell.execute_reply": "2026-02-16T03:07:07.529340Z",
     "shell.execute_reply.started": "2026-02-16T03:07:07.517690Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_with_nova(prompt: str, model_id: str, temperature: float) -> Dict:\n",
    "    enhanced_prompt = f\"\"\"{prompt}\n",
    "\n",
    "Respond with a JSON object in this exact format:\n",
    "{{\n",
    "  \"judged-compliant\": true/false,\n",
    "  \"judged-compliant-reason\": \"detailed explanation\"\n",
    "}}\"\"\"\n",
    "    \n",
    "    body = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": enhanced_prompt}]}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": MAX_TOKENS,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = bedrock_call_with_retry(\n",
    "        lambda: bedrock_runtime.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    content = response_body['output']['message']['content'][0]['text']\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    try:\n",
    "        # Find JSON in response text\n",
    "        start = content.find('{')\n",
    "        end = content.rfind('}') + 1\n",
    "        json_str = content[start:end]\n",
    "        result = json.loads(json_str)\n",
    "        \n",
    "        return {\n",
    "            \"judged-compliant\": result[\"judged-compliant\"],\n",
    "            \"judged-compliant-reason\": result[\"judged-compliant-reason\"],\n",
    "            \"llm-judge-input-tokens\": response_body.get('usage', {}).get('inputTokens', 0),\n",
    "            \"llm-judge-output-tokens\": response_body.get('usage', {}).get('outputTokens', 0),\n",
    "            \"llm-judge-total-tokens\": response_body.get('usage', {}).get('inputTokens', 0) + response_body.get('usage', {}).get('outputTokens', 0)\n",
    "        }\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        return {\n",
    "            \"judged-compliant\": False,\n",
    "            \"judged-compliant-reason\": f\"Error parsing Nova response: {str(e)}\",\n",
    "            \"llm-judge-input-tokens\": 0,\n",
    "            \"llm-judge-output-tokens\": 0,\n",
    "            \"llm-judge-total-tokens\": 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "execution_state": "idle",
   "id": "01b553aa-0099-445e-a80e-afed0fcbef57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:08.072534Z",
     "iopub.status.busy": "2026-02-16T03:07:08.072142Z",
     "iopub.status.idle": "2026-02-16T03:07:08.096249Z",
     "shell.execute_reply": "2026-02-16T03:07:08.088857Z",
     "shell.execute_reply.started": "2026-02-16T03:07:08.072484Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_scenarios(\n",
    "    source_scenarios: List[Dict],\n",
    "    model_arn: str, \n",
    "    temperature: float = None,\n",
    "    num_scenarios: int = None\n",
    ") -> List[Dict]:\n",
    "    \n",
    "    # Extract model ID and detect model type\n",
    "    model_id = model_arn.split('/')[-1] if '/' in model_arn else model_arn\n",
    "    is_nova_model = \"nova\" in model_id.lower()\n",
    "    \n",
    "    judged_scenarios = []\n",
    "    for scenario in source_scenarios[:num_scenarios] if num_scenarios else source_scenarios:\n",
    "        \n",
    "        # Get policy context from S3 for this scenario\n",
    "        retrieved_policies = get_policies_for_scenario(scenario)\n",
    "        if not retrieved_policies:\n",
    "            continue  # Skip scenarios without policy references\n",
    "        \n",
    "        prompt = COMPLIANCE_JUDGE_PROMPT.format(\n",
    "            retrieved_policies=retrieved_policies,\n",
    "            scenario_detail=scenario[\"scenario-detail\"]\n",
    "        )\n",
    "        \n",
    "        if is_nova_model:\n",
    "            # Nova: Use invoke_model with JSON response\n",
    "            judged_scenario_detail = judge_with_nova(prompt, model_id, temperature)\n",
    "            print(\"Calling judge_with_nova\")\n",
    "        else:\n",
    "            # Claude: Use converse with tools\n",
    "            judged_scenario_detail = judge_with_claude(prompt, model_id, temperature)\n",
    "            print(\"Calling judge_with_claude\")\n",
    "        \n",
    "        # Add metadata\n",
    "        judged_scenario = scenario.copy()\n",
    "        judged_scenario.update(judged_scenario_detail)\n",
    "        judged_scenario.update({\n",
    "            \"judged-dtm\": datetime.datetime.now().isoformat(),\n",
    "            \"llm-judge\": model_id,\n",
    "            \"llm-judge-temp\": temperature\n",
    "        })\n",
    "        \n",
    "        judged_scenarios.append(judged_scenario)\n",
    "    \n",
    "    return judged_scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "execution_state": "idle",
   "id": "aaf8bcc2-d0f7-40bb-8c04-17f9981de4fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:09.422783Z",
     "iopub.status.busy": "2026-02-16T03:07:09.420692Z",
     "iopub.status.idle": "2026-02-16T03:07:09.446431Z",
     "shell.execute_reply": "2026-02-16T03:07:09.437215Z",
     "shell.execute_reply.started": "2026-02-16T03:07:09.422737Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_scenarios_streaming(\n",
    "    source_scenarios: List[Dict],\n",
    "    model_arn: str, \n",
    "    temperature: float = None,\n",
    "    num_scenarios: int = None,\n",
    "    start_index: int = 0,\n",
    "    output_file: Path = None\n",
    ") -> List[Dict]:\n",
    "    \n",
    "    model_id = model_arn.split('/')[-1] if '/' in model_arn else model_arn\n",
    "    is_nova_model = \"nova\" in model_id.lower()\n",
    "    \n",
    "    # Initialize file with opening bracket\n",
    "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('{\"scenarios\":[')\n",
    "    \n",
    "    judged_scenarios = []\n",
    "    \n",
    "    if num_scenarios is not None:\n",
    "        scenarios_to_process = source_scenarios[start_index:start_index + num_scenarios]\n",
    "    else:\n",
    "        scenarios_to_process = source_scenarios[start_index:]\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios_to_process):\n",
    "        retrieved_policies = get_policies_for_scenario(scenario)\n",
    "        if not retrieved_policies:\n",
    "            continue\n",
    "        \n",
    "        prompt = COMPLIANCE_JUDGE_PROMPT.format(\n",
    "            retrieved_policies=retrieved_policies,\n",
    "            scenario_detail=scenario[\"scenario-detail\"]\n",
    "        )\n",
    "        \n",
    "        if is_nova_model:\n",
    "            judged_scenario_detail = judge_with_nova(prompt, model_id, temperature)\n",
    "        else:\n",
    "            judged_scenario_detail = judge_with_claude(prompt, model_id, temperature)\n",
    "        \n",
    "        judged_scenario = scenario.copy()\n",
    "        judged_scenario.update(judged_scenario_detail)\n",
    "        judged_scenario.update({\n",
    "            \"judged-dtm\": datetime.datetime.now().isoformat(),\n",
    "            \"llm-judge\": model_id,\n",
    "            \"llm-judge-temp\": temperature\n",
    "        })\n",
    "        \n",
    "        judged_scenarios.append(judged_scenario)\n",
    "        \n",
    "        # Append to file immediately\n",
    "        with open(output_file, 'a') as f:\n",
    "            if i > 0:  # Add comma before all but first scenario\n",
    "                f.write(',')\n",
    "            json.dump(judged_scenario, f)\n",
    "        \n",
    "        print(f\"Saved scenario {i+1}/{len(scenarios_to_process)}\")\n",
    "    \n",
    "    # Close the JSON structure\n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(']}')\n",
    "    \n",
    "    return judged_scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "execution_state": "idle",
   "id": "b17b51d7-4d3c-4258-bd1b-58e80adb8763",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:10.256168Z",
     "iopub.status.busy": "2026-02-16T03:07:10.255270Z",
     "iopub.status.idle": "2026-02-16T03:07:10.279343Z",
     "shell.execute_reply": "2026-02-16T03:07:10.275852Z",
     "shell.execute_reply.started": "2026-02-16T03:07:10.255848Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_scenarios_to_file(scenarios: List[Dict], output_path: Path):\n",
    "    \n",
    "    # Print scenarios to console for immediate review\n",
    "    print(json.dumps(scenarios, indent=2))\n",
    "\n",
    "    # Create parent directories if they don't exist\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save to file with metadata and statistics\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'total_scenarios': len(scenarios),\n",
    "            'compliant_count': sum(1 for s in scenarios if s['is-compliant']),\n",
    "            'non_compliant_count': sum(1 for s in scenarios if not s['is-compliant']),\n",
    "            'judged_compliant_count': sum(1 for s in scenarios if s['judged-compliant']),\n",
    "            'judged_non_compliant_count': sum(1 for s in scenarios if not s['judged-compliant']),\n",
    "            'scenarios': scenarios\n",
    "        }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "execution_state": "idle",
   "id": "8b5a5088-bba4-4786-82b1-44374c091b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:11.406116Z",
     "iopub.status.busy": "2026-02-16T03:07:11.405375Z",
     "iopub.status.idle": "2026-02-16T03:07:11.428389Z",
     "shell.execute_reply": "2026-02-16T03:07:11.417926Z",
     "shell.execute_reply.started": "2026-02-16T03:07:11.406065Z"
    }
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    models=None,\n",
    "    scenarios=None,\n",
    "    num_scenarios: int = None,\n",
    "    start_index: int = 0,\n",
    "):\n",
    "\n",
    "    # Start keep-alive thread\n",
    "    # daemon=True parameter makes the thread automatically stop when the main program ends\n",
    "    thread = threading.Thread(target=keep_alive, daemon=True)\n",
    "    thread.start()\n",
    "    \n",
    "    if not models or not scenarios:\n",
    "        print(\"Error: Both models and scenarios must be provided\")\n",
    "        return\n",
    "\n",
    "    # Get model names for validation\n",
    "    model_names = [m['name'] for m in MODELS]\n",
    "    \n",
    "    # Handle \"all\" and convert to lists\n",
    "    models = model_names if str(models).lower() == \"all\" else [models] if isinstance(models, str) else models\n",
    "    scenarios = SCENARIOS if str(scenarios).lower() == \"all\" else [scenarios] if isinstance(scenarios, str) else scenarios\n",
    "    \n",
    "    # Validate models\n",
    "    if invalid := [m for m in models if m not in model_names]:\n",
    "        print(f\"Invalid models: {invalid}. Valid: {model_names}\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"Processing Summary:\")\n",
    "    print(\"=\" * 100)\n",
    "    for model_name in models:\n",
    "        model = next(m for m in MODELS if m['name'] == model_name)\n",
    "        print(f\"Model: {model_name} (temp: {model['temperature']})\")\n",
    "    for scenario_file in scenarios:\n",
    "        print(f\"Scenario file: {scenario_file}\")\n",
    "    print(f\"Number of scenarios: {num_scenarios}\")\n",
    "    print(f\"Starting at scenario index: {start_index}\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Wait for user confirmation\n",
    "    input(\"Press Enter to proceed...\")\n",
    "\n",
    "    judged_scenarios_batch = \"\"\n",
    "    \n",
    "     # Run all specified combinations\n",
    "    for model_name in models:\n",
    "        for scenario_file in scenarios:\n",
    "            try:    \n",
    "                model = next(m for m in MODELS if m['name'] == model_name)\n",
    "                source_scenarios = load_scenarios_from_s3(BUCKET, S3_SOURCE_SCENARIOS, scenario_file)\n",
    "                print(f\"Processing: {model_name} with {scenario_file}\")\n",
    "            \n",
    "                judged_scenarios_batch = f\"judged_scenarios_batch-{scenario_file.split('.')[0]}-{model['name']}-temp{model['temperature']}\"\n",
    "                print (\"=\" * 100)\n",
    "                print(f\"Starting batch: {judged_scenarios_batch}\")\n",
    "                print (\"=\" * 100)\n",
    "\n",
    "                judged_scenarios_file = f\"{judged_scenarios_batch}.json\"\n",
    "                local_file_path: Path = FOLDER_JUDGED_SCENARIOS / judged_scenarios_file\n",
    "                s3_key = S3_JUDGED_SCENARIOS + judged_scenarios_file\n",
    "                \n",
    "                judged_scenarios = judge_scenarios_streaming(\n",
    "                    source_scenarios = source_scenarios,\n",
    "                    model_arn = model['arn'],\n",
    "                    temperature = model['temperature'],\n",
    "                    num_scenarios = num_scenarios,\n",
    "                    start_index = start_index,\n",
    "                    output_file = local_file_path\n",
    "                )\n",
    "\n",
    "                save_file_to_s3(local_file_path, BUCKET, s3_key)\n",
    "                \n",
    "                print (\"=\" * 100)\n",
    "                print(f\"Finished batch: {judged_scenarios_batch}: {len(judged_scenarios)} scenarios processed\")\n",
    "                print (\"=\" * 100)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {judged_scenarios_batch}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()  # show  full error trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "running",
   "id": "c0fae162-5344-421d-9ab6-29dfef33dcdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:07:12.205172Z",
     "iopub.status.busy": "2026-02-16T03:07:12.204917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Summary:\n",
      "====================================================================================================\n",
      "Model: nova_premier-t0.0 (temp: 0.0)\n",
      "Model: nova_premier-t0.1 (temp: 0.1)\n",
      "Scenario file: scenarios-8-policies-each.json\n",
      "Number of scenarios: 200\n",
      "Starting at scenario index: 200\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to proceed... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: nova_premier-t0.0 with scenarios-8-policies-each.json\n",
      "====================================================================================================\n",
      "Starting batch: judged_scenarios_batch-scenarios-8-policies-each-nova_premier-t0.0-temp0.0\n",
      "====================================================================================================\n",
      "Saved scenario 1/200\n",
      "Saved scenario 2/200\n",
      "Saved scenario 3/200\n"
     ]
    }
   ],
   "source": [
    "# main(\"all\", \"all\")\n",
    "main([\"nova_premier-t0.0\", \"nova_premier-t0.1\"], \"scenarios-8-policies-each.json\", 200, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_state": "running",
   "id": "61d791d7-018d-4391-b2b6-b5242d108ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client('bedrock', region_name=AWS_REGION)\n",
    "response = bedrock_client.list_inference_profiles()\n",
    "for profile in response['inferenceProfileSummaries']:\n",
    "    if 'meta' in profile['inferenceProfileName'].lower():\n",
    "        print(f\"ARN: {profile['inferenceProfileArn']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736c07d-3bca-4aca-848e-3e66a07f9896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
